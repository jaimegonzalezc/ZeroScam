{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "344f9d047a82415eae5d25520621937b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [],
            "layout": "IPY_MODEL_57a488f4f38d4db48ef87ee8b8988954"
          }
        },
        "47ba9942506b4f9f9a067b877decc515": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5100674e9785441bbe40b8663dafa9db",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_bc1c9151c396411e98f7efc414ab88e3",
            "value": "<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"
          }
        },
        "07d96c0bf258491480ab501c27f92dc5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "PasswordModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "PasswordModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "PasswordView",
            "continuous_update": true,
            "description": "Token:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_75ab0302fea54c05946073fdcab67348",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_66f304bcf4734f07a169c5209feee002",
            "value": ""
          }
        },
        "852d14b1d1264880a9846a211a7ef60b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "CheckboxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "CheckboxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "CheckboxView",
            "description": "Add token as git credential?",
            "description_tooltip": null,
            "disabled": false,
            "indent": true,
            "layout": "IPY_MODEL_faa7bf69e91443d8a4dedbd9e1f54b96",
            "style": "IPY_MODEL_8de4edacaa6b436ca2d8ae5c878423b3",
            "value": true
          }
        },
        "02861aa887e54e768d30d86097b73e96": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Login",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_7a674adf0ea040458fd4c081c417a110",
            "style": "IPY_MODEL_97130284ce9a4ea48ac2dd0da4c83810",
            "tooltip": ""
          }
        },
        "da88884cc0274209b132fdbaa72fa22a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bf7bfd94f9c84038a94658998a5a86d3",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_55efe188497d426f9afc6b618191ba8a",
            "value": "\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"
          }
        },
        "57a488f4f38d4db48ef87ee8b8988954": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "center",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "5100674e9785441bbe40b8663dafa9db": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bc1c9151c396411e98f7efc414ab88e3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "75ab0302fea54c05946073fdcab67348": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "66f304bcf4734f07a169c5209feee002": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "faa7bf69e91443d8a4dedbd9e1f54b96": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8de4edacaa6b436ca2d8ae5c878423b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7a674adf0ea040458fd4c081c417a110": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "97130284ce9a4ea48ac2dd0da4c83810": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "bf7bfd94f9c84038a94658998a5a86d3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "55efe188497d426f9afc6b618191ba8a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6836bdc7830e4fc7b0c6e7c6f2281f4a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4922ea9df59443008a73f8cb1f258bfe",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_1870d1bf5715423a991e960a3b2e77c9",
            "value": "Connecting..."
          }
        },
        "4922ea9df59443008a73f8cb1f258bfe": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1870d1bf5715423a991e960a3b2e77c9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e9f1a86ea76e4d73b787fac08f597db7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5060ef17028249f783f8c961f6df5e6a",
              "IPY_MODEL_423f7d49213241bd9daf4fa7f083cb10",
              "IPY_MODEL_9e52930293164fdd904f3205c20702fb"
            ],
            "layout": "IPY_MODEL_ed6ff4a017384d848f4ec9e2b8fb4200"
          }
        },
        "5060ef17028249f783f8c961f6df5e6a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b3c5453e3a9942e69b8e5abb6e09937c",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_7f1d6aa2c1fd4424a2d5bc911ee127aa",
            "value": "Loading‚Äácheckpoint‚Äáshards:‚Äá100%"
          }
        },
        "423f7d49213241bd9daf4fa7f083cb10": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_413ce24651c849aeb1bce9ba6da909b5",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_19c071aeac61459c948ba8ce6689460e",
            "value": 2
          }
        },
        "9e52930293164fdd904f3205c20702fb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cefd59584a764cb08f4f2ecf2795ee2d",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_638a924f8e09441cbde08de9051d637e",
            "value": "‚Äá2/2‚Äá[00:09&lt;00:00,‚Äá‚Äá4.84s/it]"
          }
        },
        "ed6ff4a017384d848f4ec9e2b8fb4200": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b3c5453e3a9942e69b8e5abb6e09937c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7f1d6aa2c1fd4424a2d5bc911ee127aa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "413ce24651c849aeb1bce9ba6da909b5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "19c071aeac61459c948ba8ce6689460e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "cefd59584a764cb08f4f2ecf2795ee2d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "638a924f8e09441cbde08de9051d637e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#ZeroScam\n",
        "\n"
      ],
      "metadata": {
        "id": "DQrdF4Xi82_f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Instalo librerias"
      ],
      "metadata": {
        "id": "-N0vGEj1BX1k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install chromadb fastapi pyngrok uvicorn\n",
        "!pip install datasets trl\n",
        "!pip uninstall -y bitsandbytes\n",
        "!pip install --upgrade bitsandbytes\n",
        "!pip install --upgrade transformers accelerate\n",
        "!nvidia-smi\n",
        "!pip install accelerate\n",
        "!accelerate config\n",
        "!pip install flash-attn --no-build-isolation\n",
        "!pip install pytesseract\n",
        "!pip install --upgrade python-telegram-bot\n",
        "!pip install dotenv\n",
        "!apt-get update\n",
        "!apt-get install -y tesseract-ocr\n",
        "!pip install pytesseract\n",
        "!tesseract -v"
      ],
      "metadata": {
        "id": "MVMdkCQBHeHj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import re\n",
        "import sys\n",
        "import threading\n",
        "import logging\n",
        "import requests\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from bitsandbytes import BitsAndBytesConfig\n",
        "\n",
        "from fastapi import FastAPI\n",
        "import uvicorn\n",
        "from pyngrok import ngrok\n",
        "\n",
        "import pytesseract\n",
        "from PIL import Image, ImageEnhance\n",
        "import spacy\n",
        "\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()"
      ],
      "metadata": {
        "id": "Erhv3xLHpaa1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CONFIGURACI√ìN: Montar Google Drive y Variables\n",
        "drive.mount('/content/drive')\n",
        "BASE_DIR = \"/content/drive/My Drive/Trabajo Final Bootcamp\"\n",
        "NORMATIVA_DIR = os.path.join(BASE_DIR, \"normativa\")\n",
        "EMBEDDINGS_BACKUP_PATH = os.path.join(BASE_DIR, \"embeddings.json\")\n",
        "\n",
        "if not os.path.exists(NORMATIVA_DIR):\n",
        "    raise FileNotFoundError(f\"La carpeta {NORMATIVA_DIR} no existe. Verifica la ruta o crea la carpeta.\")"
      ],
      "metadata": {
        "id": "BN1PvRbIpcAs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Login en Hugging Face\n",
        "notebook_login()"
      ],
      "metadata": {
        "id": "clV8Z482pg--"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CARGA DE MODELOS Y CONFIGURACI√ìN DE DISPOSITIVO\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"‚úÖ Usando dispositivo: {device}\")\n",
        "\n",
        "model_name = \"CasiAC/deepseek-r1-8b-ciberseguridad\"\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\"\n",
        ")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "embedding_model = SentenceTransformer('all-MiniLM-L6-v2', device=device)\n",
        "print(\"‚úÖ Modelo cargado üöÄ\")"
      ],
      "metadata": {
        "id": "0pSHhiLwpqTF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CONFIGURACI√ìN DE CHROMADB Y FASTAPI\n",
        "app = FastAPI()\n",
        "\n",
        "# Configuraci√≥n de ngrok para exponer el puerto 8000\n",
        "ngrok.set_auth_token(\"2sN2ljFFRN4UpJk7VPL6jPiHVJL_6FafRKrvugJysTGGRV1KB\")\n",
        "public_url = ngrok.connect(8000)\n",
        "print(f\"üîó ChromaDB API accesible en: {public_url}\")\n",
        "\n",
        "client = chromadb.Client()\n",
        "collection = client.create_collection(\n",
        "    name=\"test\",\n",
        "    metadata={\"hnsw:search_ef\": 100, \"hnsw:construction_ef\": 1000}\n",
        ")\n",
        "\n",
        "@app.get(\"/\")\n",
        "def read_root():\n",
        "    return {\"message\": \"Chroma API est√° en funcionamiento\"}\n",
        "\n",
        "@app.get(\"/collections\")\n",
        "def get_collections():\n",
        "    return {\"collections\": client.list_collections()}\n",
        "\n",
        "@app.get(\"/collections/{collection_name}\")\n",
        "def get_collection(collection_name: str):\n",
        "    return {\"collection\": client.get_collection(name=collection_name)}\n",
        "\n",
        "@app.post(\"/collections/{collection_name}/add\")\n",
        "def add_to_collection(collection_name: str, item: dict):\n",
        "    coll = client.get_collection(name=collection_name)\n",
        "    coll.add(\n",
        "        documents=[item[\"document\"]],\n",
        "        metadatas=[item.get(\"metadata\", {})],\n",
        "        ids=[item.get(\"id\", \"default_id\")]\n",
        "    )\n",
        "    return {\"message\": f\"Elemento a√±adido a la colecci√≥n {collection_name}\"}\n",
        "\n",
        "@app.get(\"/health\")\n",
        "def health_check():\n",
        "    return {\"status\": \"OK\"}\n",
        "\n",
        "def start_server():\n",
        "    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n",
        "\n",
        "server_thread = threading.Thread(target=start_server, daemon=True)\n",
        "server_thread.start()"
      ],
      "metadata": {
        "id": "Vn0w9XXdpqKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# FUNCIONES DE INDEXACI√ìN Y GENERACI√ìN DE CONTEXTO (RAG)\n",
        "def cargar_documentos_y_embeddings():\n",
        "    embeddings_dict = {}\n",
        "    for archivo in os.listdir(NORMATIVA_DIR):\n",
        "        ruta_json = os.path.join(NORMATIVA_DIR, archivo)\n",
        "        with open(ruta_json, \"r\", encoding=\"utf-8\") as f:\n",
        "            documentos = json.load(f)\n",
        "\n",
        "        for section in documentos.get(\"sections\", []):\n",
        "            doc_id = f\"{archivo}_p{section['page']}\"\n",
        "            content = section[\"content\"]\n",
        "            embedding = embedding_model.encode(content, convert_to_tensor=True).cpu().numpy().tolist()\n",
        "            embeddings_dict[doc_id] = embedding\n",
        "\n",
        "            collection.add(\n",
        "                documents=[content],\n",
        "                metadatas=[{\"title\": documentos.get(\"title\", \"Desconocido\"), \"page\": section[\"page\"]}],\n",
        "                embeddings=[embedding],\n",
        "                ids=[doc_id]\n",
        "            )\n",
        "\n",
        "    with open(EMBEDDINGS_BACKUP_PATH, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(embeddings_dict, f, ensure_ascii=False, indent=4)\n",
        "    print(f\"‚úÖ {len(embeddings_dict)} documentos indexados y guardados en ChromaDB üöÄ\")\n",
        "    return embeddings_dict\n",
        "\n",
        "embeddings_dict = cargar_documentos_y_embeddings()\n",
        "\n",
        "def obtener_contexto(pregunta, n_docs=3):\n",
        "    embedding_pregunta = embedding_model.encode([pregunta], convert_to_tensor=True).cpu().numpy().tolist()\n",
        "    resultados = collection.query(query_embeddings=embedding_pregunta, n_results=n_docs)\n",
        "    documents = resultados.get('documents', [])\n",
        "    if not documents:\n",
        "        return \"No se encontraron documentos relevantes.\"\n",
        "\n",
        "    documentos_convertidos = [\n",
        "        \" \".join(map(str, doc)) if isinstance(doc, list) else str(doc)\n",
        "        for doc in documents\n",
        "    ]\n",
        "    return \"\\n\".join(documentos_convertidos)\n",
        "\n",
        "def generar_respuesta_rag(pregunta, max_tokens=300, temperatura=0.1):\n",
        "    contexto = obtener_contexto(pregunta)\n",
        "    entrada = f\"Contexto: {contexto}\\nPregunta: {pregunta}\\nRespuesta:\"\n",
        "    inputs = tokenizer(entrada, return_tensors=\"pt\").to(device)\n",
        "    output = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=max_tokens,\n",
        "        temperature=temperatura,\n",
        "        top_p=0.9,\n",
        "        repetition_penalty=1.05\n",
        "    )\n",
        "    return tokenizer.decode(output[0], skip_special_tokens=True)"
      ],
      "metadata": {
        "id": "leD3qn6epp8V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# FUNCIONES DE OCR Y CONSULTA A VIRUSTOTAL\n",
        "pytesseract.pytesseract.tesseract_cmd = '/usr/bin/tesseract'\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "API_KEY = \"06858db9f480b4aba21a5831457a9b919b1f9014e6f8872ee1f4f7d1a029197c\"\n",
        "HEADERS = {\"x-apikey\": API_KEY}\n",
        "\n",
        "def preprocesar_imagen(imagen):\n",
        "    \"\"\"Convierte la imagen a escala de grises y mejora el contraste.\"\"\"\n",
        "    try:\n",
        "        image = Image.open(imagen).convert(\"L\")\n",
        "        enhancer = ImageEnhance.Contrast(image)\n",
        "        return enhancer.enhance(2.0)\n",
        "    except Exception as e:\n",
        "        return None\n",
        "\n",
        "def extraer_texto_img(imagen):\n",
        "    \"\"\"Extrae y limpia el texto de una imagen.\"\"\"\n",
        "    image = preprocesar_imagen(imagen)\n",
        "    if image is None:\n",
        "        return \"Error al procesar la imagen\"\n",
        "    texto_extraido = pytesseract.image_to_string(image)\n",
        "    return limpiar_texto(texto_extraido)\n",
        "\n",
        "def consultar_ip(ip):\n",
        "    url = f\"https://www.virustotal.com/api/v3/ip_addresses/{ip}\"\n",
        "    response = requests.get(url, headers=HEADERS)\n",
        "    if response.status_code == 200:\n",
        "        data = response.json()\n",
        "        stats = data[\"data\"][\"attributes\"][\"last_analysis_stats\"]\n",
        "        malicious = stats.get(\"malicious\", 0)\n",
        "        if malicious > 0:\n",
        "            veredicto = f\"‚ùå La IP {ip} ha sido reportada como maliciosa en {malicious} an√°lisis.\"\n",
        "        else:\n",
        "            veredicto = f\"‚úÖ La IP {ip} parece segura.\"\n",
        "        return {\"IP\": ip, \"Veredicto\": veredicto, \"An√°lisis\": stats}\n",
        "    return {\"error\": f\"Error en la consulta: {response.status_code}\"}\n",
        "\n",
        "def consultar_url(url):\n",
        "    scan_url = \"https://www.virustotal.com/api/v3/urls\"\n",
        "    response = requests.post(scan_url, headers=HEADERS, data={\"url\": url})\n",
        "    if response.status_code == 200:\n",
        "        analysis_id = response.json()[\"data\"][\"id\"]\n",
        "        result_url = f\"https://www.virustotal.com/api/v3/analyses/{analysis_id}\"\n",
        "        result_response = requests.get(result_url, headers=HEADERS)\n",
        "        if result_response.status_code == 200:\n",
        "            data = result_response.json()\n",
        "            stats = data[\"data\"][\"attributes\"][\"stats\"]\n",
        "            malicious = stats.get(\"malicious\", 0)\n",
        "            if malicious > 0:\n",
        "                veredicto = f\"‚ùå La URL {url} ha sido marcada como maliciosa en {malicious} an√°lisis.\"\n",
        "            else:\n",
        "                veredicto = f\"‚úÖ La URL {url} parece segura.\"\n",
        "            return {\"URL\": url, \"Veredicto\": veredicto, \"An√°lisis\": stats}\n",
        "    return {\"error\": f\"Error en la consulta: {response.status_code}\"}\n",
        "\n",
        "def limpiar_texto(texto):\n",
        "    texto = texto.strip()\n",
        "    texto = re.sub(r'\\s+', ' ', texto)\n",
        "    return texto.replace(\"\\n\", \" \").replace(\"\\r\", \"\")\n",
        "\n",
        "def analizar_con_modelo(texto_extraido):\n",
        "    \"\"\"Analiza el mensaje para detectar se√±ales de phishing utilizando el modelo.\n",
        "       Responde siempre en espa√±ol.\n",
        "    \"\"\"\n",
        "    texto_limpio = limpiar_texto(texto_extraido)\n",
        "    system_prompt = \"\"\"\n",
        "      Eres un asistente altamente especializado en ciberseguridad. Tu tarea principal es analizar mensajes y detectar intentos de phishing con precisi√≥n.\n",
        "      üîπ REGLAS ESTRICTAS:\n",
        "      1. No inventes informaci√≥n. Basa tu respuesta √öNICAMENTE en el texto proporcionado.\n",
        "      2. S√© conciso y preciso.\n",
        "      3. La respuesta debe comenzar con \"Phishing\" o \"Not Phishing\", seguido de una breve explicaci√≥n.\n",
        "      4. Responde siempre en espa√±ol.\n",
        "    \"\"\"\n",
        "    prompt_modelo = f\"\"\"\n",
        "      Analiza el siguiente mensaje y determina si se trata de un intento de phishing.\n",
        "      MENSAJE A EVALUAR:\n",
        "      {texto_extraido}\n",
        "    \"\"\"\n",
        "    inputs = tokenizer(system_prompt + prompt_modelo, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
        "    outputs = model.generate(\n",
        "        inputs.input_ids,\n",
        "        attention_mask=inputs.attention_mask,\n",
        "        max_new_tokens=300,\n",
        "        temperature=0.15,\n",
        "        do_sample=True,\n",
        "        top_k=10,\n",
        "        top_p=0.7,\n",
        "        repetition_penalty=1.2,\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        "    )\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True).strip()\n",
        "    return response\n",
        "\n",
        "def analizar_prompt(prompt):\n",
        "    \"\"\"\n",
        "    Detecta si el prompt contiene:\n",
        "      - Una imagen (por extensi√≥n) para procesar OCR y an√°lisis de phishing.\n",
        "      - Una IP o URL para consulta en VirusTotal.\n",
        "    Devuelve un resultado especial (no None) si se cumple alguno de estos casos.\n",
        "    \"\"\"\n",
        "    if isinstance(prompt, str) and prompt.lower().endswith(('.png', '.jpg', '.jpeg', '.gif', '.bmp', '.webp', '.heic')):\n",
        "        print(f\"üîç Detectada imagen: {prompt}\")\n",
        "        texto_extraido = extraer_texto_img(prompt)\n",
        "        print(f\"Texto extra√≠do: {texto_extraido}\")\n",
        "        return analizar_con_modelo(texto_extraido)\n",
        "\n",
        "    ip_pattern = r\"\\b(?:\\d{1,3}\\.){3}\\d{1,3}\\b\"\n",
        "    url_pattern = r'(https?://[^\\s]+|www\\.[^\\s]+)'\n",
        "    if re.search(ip_pattern, prompt):\n",
        "        ip = re.search(ip_pattern, prompt).group()\n",
        "        print(f\"üîç Detectada IP: {ip}\")\n",
        "        return consultar_ip(ip)\n",
        "    if re.search(url_pattern, prompt):\n",
        "        url = re.search(url_pattern, prompt).group()\n",
        "        print(f\"üîç Detectada URL: {url}\")\n",
        "        return consultar_url(url)\n",
        "    return None"
      ],
      "metadata": {
        "id": "eFZtEvzSp1cX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# FUNCI√ìN UNIFICADA DE GENERACI√ìN DE RESPUESTA\n",
        "def generate_response(prompt):\n",
        "    \"\"\"\n",
        "    Funci√≥n unificada que:\n",
        "      1. Verifica si el prompt es especial (imagen, IP, URL) mediante analizar_prompt().\n",
        "      2. Si no es especial, utiliza la generaci√≥n basada en contexto (RAG).\n",
        "      3. Y si no se encuentra relaci√≥n en el contexto (por ejemplo, se obtiene \"No se encontraron documentos relevantes\"),\n",
        "         se usa una generaci√≥n ‚Äúpor defecto‚Äù.\n",
        "    \"\"\"\n",
        "    # Paso 1: Verificar si el prompt es especial\n",
        "    resultado_api = analizar_prompt(prompt)\n",
        "    if resultado_api is not None:\n",
        "        return json.dumps(resultado_api, indent=4, ensure_ascii=False)\n",
        "\n",
        "    # Paso 2: Consultar contexto en ChromaDB para RAG\n",
        "    contexto = obtener_contexto(prompt)\n",
        "    if contexto.strip().lower().startswith(\"no se encontraron documentos\"):\n",
        "        # Paso 3: Generaci√≥n por defecto si no hay contexto relevante\n",
        "        inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
        "        outputs = model.generate(\n",
        "            inputs.input_ids,\n",
        "            attention_mask=inputs.attention_mask,\n",
        "            max_new_tokens=200,\n",
        "            temperature=0.2,\n",
        "            do_sample=True,\n",
        "            top_k=50,\n",
        "            top_p=0.9,\n",
        "            repetition_penalty=1.5,\n",
        "            pad_token_id=tokenizer.eos_token_id,\n",
        "        )\n",
        "        return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    else:\n",
        "        # Si se encontr√≥ contexto, usar generaci√≥n basada en RAG\n",
        "        return generar_respuesta_rag(prompt)\n"
      ],
      "metadata": {
        "id": "j7Wz4ayLp1PF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PRUEBA DE LA FUNCI√ìN UNIFICADA\n",
        "# ---------------------------------\n",
        "prompt_usuario = 'image6.png'\n",
        "respuesta_usuario = generate_response(prompt_usuario)\n",
        "print(f\"\\nüîπ Respuesta:\\n{respuesta_usuario}\")"
      ],
      "metadata": {
        "id": "ceOaDqvuqrX3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CONFIGURACI√ìN DEL BOT DE TELEGRAM\n",
        "# ---------------------------------\n",
        "load_dotenv()\n",
        "\n",
        "logging.basicConfig(\n",
        "    format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\",\n",
        "    level=logging.INFO,\n",
        "    handlers=[logging.StreamHandler(sys.stdout)]\n",
        ")\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "from telegram import Update\n",
        "from telegram.ext import Application, CommandHandler, MessageHandler, filters, CallbackContext\n",
        "\n",
        "async def start(update: Update, context: CallbackContext) -> None:\n",
        "    await update.message.reply_text(\"¬°Hola! Soy tu especialista en ciberseguridad. ¬øEn qu√© te puedo ayudar?\")\n",
        "\n",
        "async def handle_message(update: Update, context: CallbackContext) -> None:\n",
        "    user_input = update.message.text\n",
        "    username = update.message.from_user.username\n",
        "    logger.info(f\"Mensaje recibido de {username}: {user_input}\")\n",
        "    response = generate_response(user_input)\n",
        "    await update.message.reply_text(response)\n",
        "\n",
        "def main():\n",
        "    TELEGRAM_TOKEN = \"7047664203:AAEa-JEcZQpv-tDCIdV6ZE_odp4lPTH0Bd8\"\n",
        "    if not TELEGRAM_TOKEN:\n",
        "        logger.error(\"El token de Telegram no est√° configurado.\")\n",
        "        sys.exit(1)\n",
        "\n",
        "    app_telegram = Application.builder().token(TELEGRAM_TOKEN).build()\n",
        "    app_telegram.add_handler(CommandHandler(\"start\", start))\n",
        "    app_telegram.add_handler(MessageHandler(filters.TEXT & ~filters.COMMAND, handle_message))\n",
        "    logger.info(\"Bot iniciado y ejecut√°ndose...\")\n",
        "    app_telegram.run_polling()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "2ULcOY7Tq3c3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "L46NcVtWrR9O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "l2qe-grSrRjv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4eo1Rf7TrRSB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import re\n",
        "import sys\n",
        "import threading\n",
        "import logging\n",
        "import requests\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from bitsandbytes import BitsAndBytesConfig\n",
        "\n",
        "from fastapi import FastAPI\n",
        "import uvicorn\n",
        "from pyngrok import ngrok\n",
        "\n",
        "import pytesseract\n",
        "from PIL import Image, ImageEnhance\n",
        "import spacy\n",
        "\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# ---------------------------------\n",
        "# CONFIGURACI√ìN: Montar Google Drive y Variables\n",
        "# ---------------------------------\n",
        "drive.mount('/content/drive')\n",
        "BASE_DIR = \"/content/drive/My Drive/Trabajo Final Bootcamp\"\n",
        "NORMATIVA_DIR = os.path.join(BASE_DIR, \"normativa\")\n",
        "EMBEDDINGS_BACKUP_PATH = os.path.join(BASE_DIR, \"embeddings.json\")\n",
        "\n",
        "if not os.path.exists(NORMATIVA_DIR):\n",
        "    raise FileNotFoundError(f\"La carpeta {NORMATIVA_DIR} no existe. Verifica la ruta o crea la carpeta.\")\n",
        "\n",
        "notebook_login()  # Login en Hugging Face (ejecutar solo una vez)\n",
        "\n",
        "# ---------------------------------\n",
        "# CARGA DE MODELOS Y CONFIGURACI√ìN DE DISPOSITIVO\n",
        "# ---------------------------------\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"‚úÖ Usando dispositivo: {device}\")\n",
        "\n",
        "model_name = \"CasiAC/deepseek-r1-8b-ciberseguridad\"\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\"\n",
        ")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "embedding_model = SentenceTransformer('all-MiniLM-L6-v2', device=device)\n",
        "print(\"‚úÖ Modelo cargado üöÄ\")\n",
        "\n",
        "# ---------------------------------\n",
        "# CONFIGURACI√ìN DE CHROMADB Y FASTAPI\n",
        "# ---------------------------------\n",
        "app = FastAPI()\n",
        "\n",
        "# Configuraci√≥n de ngrok para exponer el puerto 8000\n",
        "ngrok.set_auth_token(\"2sN2ljFFRN4UpJk7VPL6jPiHVJL_6FafRKrvugJysTGGRV1KB\")\n",
        "public_url = ngrok.connect(8000)\n",
        "print(f\"üîó ChromaDB API accesible en: {public_url}\")\n",
        "\n",
        "client = chromadb.Client()\n",
        "collection = client.create_collection(\n",
        "    name=\"test\",\n",
        "    metadata={\"hnsw:search_ef\": 100, \"hnsw:construction_ef\": 1000}\n",
        ")\n",
        "\n",
        "@app.get(\"/\")\n",
        "def read_root():\n",
        "    return {\"message\": \"Chroma API est√° en funcionamiento\"}\n",
        "\n",
        "@app.get(\"/collections\")\n",
        "def get_collections():\n",
        "    return {\"collections\": client.list_collections()}\n",
        "\n",
        "@app.get(\"/collections/{collection_name}\")\n",
        "def get_collection(collection_name: str):\n",
        "    return {\"collection\": client.get_collection(name=collection_name)}\n",
        "\n",
        "@app.post(\"/collections/{collection_name}/add\")\n",
        "def add_to_collection(collection_name: str, item: dict):\n",
        "    coll = client.get_collection(name=collection_name)\n",
        "    coll.add(\n",
        "        documents=[item[\"document\"]],\n",
        "        metadatas=[item.get(\"metadata\", {})],\n",
        "        ids=[item.get(\"id\", \"default_id\")]\n",
        "    )\n",
        "    return {\"message\": f\"Elemento a√±adido a la colecci√≥n {collection_name}\"}\n",
        "\n",
        "@app.get(\"/health\")\n",
        "def health_check():\n",
        "    return {\"status\": \"OK\"}\n",
        "\n",
        "def start_server():\n",
        "    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n",
        "\n",
        "server_thread = threading.Thread(target=start_server, daemon=True)\n",
        "server_thread.start()\n",
        "\n",
        "# ---------------------------------\n",
        "# FUNCIONES DE INDEXACI√ìN Y GENERACI√ìN DE CONTEXTO (RAG)\n",
        "# ---------------------------------\n",
        "def cargar_documentos_y_embeddings():\n",
        "    embeddings_dict = {}\n",
        "    for archivo in os.listdir(NORMATIVA_DIR):\n",
        "        ruta_json = os.path.join(NORMATIVA_DIR, archivo)\n",
        "        with open(ruta_json, \"r\", encoding=\"utf-8\") as f:\n",
        "            documentos = json.load(f)\n",
        "\n",
        "        for section in documentos.get(\"sections\", []):\n",
        "            doc_id = f\"{archivo}_p{section['page']}\"\n",
        "            content = section[\"content\"]\n",
        "            embedding = embedding_model.encode(content, convert_to_tensor=True).cpu().numpy().tolist()\n",
        "            embeddings_dict[doc_id] = embedding\n",
        "\n",
        "            collection.add(\n",
        "                documents=[content],\n",
        "                metadatas=[{\"title\": documentos.get(\"title\", \"Desconocido\"), \"page\": section[\"page\"]}],\n",
        "                embeddings=[embedding],\n",
        "                ids=[doc_id]\n",
        "            )\n",
        "\n",
        "    with open(EMBEDDINGS_BACKUP_PATH, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(embeddings_dict, f, ensure_ascii=False, indent=4)\n",
        "    print(f\"‚úÖ {len(embeddings_dict)} documentos indexados y guardados en ChromaDB üöÄ\")\n",
        "    return embeddings_dict\n",
        "\n",
        "embeddings_dict = cargar_documentos_y_embeddings()\n",
        "\n",
        "def obtener_contexto(pregunta, n_docs=3):\n",
        "    embedding_pregunta = embedding_model.encode([pregunta], convert_to_tensor=True).cpu().numpy().tolist()\n",
        "    resultados = collection.query(query_embeddings=embedding_pregunta, n_results=n_docs)\n",
        "    documents = resultados.get('documents', [])\n",
        "    if not documents:\n",
        "        return \"No se encontraron documentos relevantes.\"\n",
        "\n",
        "    documentos_convertidos = [\n",
        "        \" \".join(map(str, doc)) if isinstance(doc, list) else str(doc)\n",
        "        for doc in documents\n",
        "    ]\n",
        "    return \"\\n\".join(documentos_convertidos)\n",
        "\n",
        "def generar_respuesta_rag(pregunta, max_tokens=300, temperatura=0.1):\n",
        "    contexto = obtener_contexto(pregunta)\n",
        "    entrada = f\"Contexto: {contexto}\\nPregunta: {pregunta}\\nRespuesta:\"\n",
        "    inputs = tokenizer(entrada, return_tensors=\"pt\").to(device)\n",
        "    output = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=max_tokens,\n",
        "        temperature=temperatura,\n",
        "        top_p=0.9,\n",
        "        repetition_penalty=1.05\n",
        "    )\n",
        "    return tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "# ---------------------------------\n",
        "# FUNCIONES DE OCR Y CONSULTA A VIRUSTOTAL\n",
        "# ---------------------------------\n",
        "pytesseract.pytesseract.tesseract_cmd = '/usr/bin/tesseract'\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "API_KEY = \"06858db9f480b4aba21a5831457a9b919b1f9014e6f8872ee1f4f7d1a029197c\"\n",
        "HEADERS = {\"x-apikey\": API_KEY}\n",
        "\n",
        "def preprocesar_imagen(imagen):\n",
        "    \"\"\"Convierte la imagen a escala de grises y mejora el contraste.\"\"\"\n",
        "    try:\n",
        "        image = Image.open(imagen).convert(\"L\")\n",
        "        enhancer = ImageEnhance.Contrast(image)\n",
        "        return enhancer.enhance(2.0)\n",
        "    except Exception as e:\n",
        "        return None\n",
        "\n",
        "def extraer_texto_img(imagen):\n",
        "    \"\"\"Extrae y limpia el texto de una imagen.\"\"\"\n",
        "    image = preprocesar_imagen(imagen)\n",
        "    if image is None:\n",
        "        return \"Error al procesar la imagen\"\n",
        "    texto_extraido = pytesseract.image_to_string(image)\n",
        "    return limpiar_texto(texto_extraido)\n",
        "\n",
        "def consultar_ip(ip):\n",
        "    url = f\"https://www.virustotal.com/api/v3/ip_addresses/{ip}\"\n",
        "    response = requests.get(url, headers=HEADERS)\n",
        "    if response.status_code == 200:\n",
        "        data = response.json()\n",
        "        stats = data[\"data\"][\"attributes\"][\"last_analysis_stats\"]\n",
        "        malicious = stats.get(\"malicious\", 0)\n",
        "        if malicious > 0:\n",
        "            veredicto = f\"‚ùå La IP {ip} ha sido reportada como maliciosa en {malicious} an√°lisis.\"\n",
        "        else:\n",
        "            veredicto = f\"‚úÖ La IP {ip} parece segura.\"\n",
        "        return {\"IP\": ip, \"Veredicto\": veredicto, \"An√°lisis\": stats}\n",
        "    return {\"error\": f\"Error en la consulta: {response.status_code}\"}\n",
        "\n",
        "def consultar_url(url):\n",
        "    scan_url = \"https://www.virustotal.com/api/v3/urls\"\n",
        "    response = requests.post(scan_url, headers=HEADERS, data={\"url\": url})\n",
        "    if response.status_code == 200:\n",
        "        analysis_id = response.json()[\"data\"][\"id\"]\n",
        "        result_url = f\"https://www.virustotal.com/api/v3/analyses/{analysis_id}\"\n",
        "        result_response = requests.get(result_url, headers=HEADERS)\n",
        "        if result_response.status_code == 200:\n",
        "            data = result_response.json()\n",
        "            stats = data[\"data\"][\"attributes\"][\"stats\"]\n",
        "            malicious = stats.get(\"malicious\", 0)\n",
        "            if malicious > 0:\n",
        "                veredicto = f\"‚ùå La URL {url} ha sido marcada como maliciosa en {malicious} an√°lisis.\"\n",
        "            else:\n",
        "                veredicto = f\"‚úÖ La URL {url} parece segura.\"\n",
        "            return {\"URL\": url, \"Veredicto\": veredicto, \"An√°lisis\": stats}\n",
        "    return {\"error\": f\"Error en la consulta: {response.status_code}\"}\n",
        "\n",
        "def limpiar_texto(texto):\n",
        "    texto = texto.strip()\n",
        "    texto = re.sub(r'\\s+', ' ', texto)\n",
        "    return texto.replace(\"\\n\", \" \").replace(\"\\r\", \"\")\n",
        "\n",
        "def analizar_con_modelo(texto_extraido):\n",
        "    \"\"\"Analiza el mensaje para detectar se√±ales de phishing utilizando el modelo.\n",
        "       Responde siempre en espa√±ol.\n",
        "    \"\"\"\n",
        "    texto_limpio = limpiar_texto(texto_extraido)\n",
        "    system_prompt = \"\"\"\n",
        "      Eres un asistente altamente especializado en ciberseguridad. Tu tarea principal es analizar mensajes y detectar intentos de phishing con precisi√≥n.\n",
        "      üîπ REGLAS ESTRICTAS:\n",
        "      1. No inventes informaci√≥n. Basa tu respuesta √öNICAMENTE en el texto proporcionado.\n",
        "      2. S√© conciso y preciso.\n",
        "      3. La respuesta debe comenzar con \"Phishing\" o \"Not Phishing\", seguido de una breve explicaci√≥n.\n",
        "      4. Responde siempre en espa√±ol.\n",
        "    \"\"\"\n",
        "    prompt_modelo = f\"\"\"\n",
        "      Analiza el siguiente mensaje y determina si se trata de un intento de phishing.\n",
        "      MENSAJE A EVALUAR:\n",
        "      {texto_extraido}\n",
        "    \"\"\"\n",
        "    inputs = tokenizer(system_prompt + prompt_modelo, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
        "    outputs = model.generate(\n",
        "        inputs.input_ids,\n",
        "        attention_mask=inputs.attention_mask,\n",
        "        max_new_tokens=300,\n",
        "        temperature=0.15,\n",
        "        do_sample=True,\n",
        "        top_k=10,\n",
        "        top_p=0.7,\n",
        "        repetition_penalty=1.2,\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        "    )\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True).strip()\n",
        "    return response\n",
        "\n",
        "def analizar_prompt(prompt):\n",
        "    \"\"\"\n",
        "    Detecta si el prompt contiene:\n",
        "      - Una imagen (por extensi√≥n) para procesar OCR y an√°lisis de phishing.\n",
        "      - Una IP o URL para consulta en VirusTotal.\n",
        "    Devuelve un resultado especial (no None) si se cumple alguno de estos casos.\n",
        "    \"\"\"\n",
        "    if isinstance(prompt, str) and prompt.lower().endswith(('.png', '.jpg', '.jpeg', '.gif', '.bmp', '.webp', '.heic')):\n",
        "        print(f\"üîç Detectada imagen: {prompt}\")\n",
        "        texto_extraido = extraer_texto_img(prompt)\n",
        "        print(f\"Texto extra√≠do: {texto_extraido}\")\n",
        "        return analizar_con_modelo(texto_extraido)\n",
        "\n",
        "    ip_pattern = r\"\\b(?:\\d{1,3}\\.){3}\\d{1,3}\\b\"\n",
        "    url_pattern = r'(https?://[^\\s]+|www\\.[^\\s]+)'\n",
        "    if re.search(ip_pattern, prompt):\n",
        "        ip = re.search(ip_pattern, prompt).group()\n",
        "        print(f\"üîç Detectada IP: {ip}\")\n",
        "        return consultar_ip(ip)\n",
        "    if re.search(url_pattern, prompt):\n",
        "        url = re.search(url_pattern, prompt).group()\n",
        "        print(f\"üîç Detectada URL: {url}\")\n",
        "        return consultar_url(url)\n",
        "    return None\n",
        "\n",
        "# ---------------------------------\n",
        "# FUNCI√ìN UNIFICADA DE GENERACI√ìN DE RESPUESTA\n",
        "# ---------------------------------\n",
        "def generate_response(prompt):\n",
        "    \"\"\"\n",
        "    Funci√≥n unificada que:\n",
        "      1. Verifica si el prompt es especial (imagen, IP, URL) mediante analizar_prompt().\n",
        "      2. Si no es especial, utiliza la generaci√≥n basada en contexto (RAG).\n",
        "      3. Y si no se encuentra relaci√≥n en el contexto (por ejemplo, se obtiene \"No se encontraron documentos relevantes\"),\n",
        "         se usa una generaci√≥n ‚Äúpor defecto‚Äù.\n",
        "    \"\"\"\n",
        "    # Paso 1: Verificar si el prompt es especial\n",
        "    resultado_api = analizar_prompt(prompt)\n",
        "    if resultado_api is not None:\n",
        "        return json.dumps(resultado_api, indent=4, ensure_ascii=False)\n",
        "\n",
        "    # Paso 2: Consultar contexto en ChromaDB para RAG\n",
        "    contexto = obtener_contexto(prompt)\n",
        "    if contexto.strip().lower().startswith(\"no se encontraron documentos\"):\n",
        "        # Paso 3: Generaci√≥n por defecto si no hay contexto relevante\n",
        "        inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
        "        outputs = model.generate(\n",
        "            inputs.input_ids,\n",
        "            attention_mask=inputs.attention_mask,\n",
        "            max_new_tokens=200,\n",
        "            temperature=0.2,\n",
        "            do_sample=True,\n",
        "            top_k=50,\n",
        "            top_p=0.9,\n",
        "            repetition_penalty=1.5,\n",
        "            pad_token_id=tokenizer.eos_token_id,\n",
        "        )\n",
        "        return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    else:\n",
        "        # Si se encontr√≥ contexto, usar generaci√≥n basada en RAG\n",
        "        return generar_respuesta_rag(prompt)\n",
        "\n",
        "# ---------------------------------\n",
        "# PRUEBA DE LA FUNCI√ìN UNIFICADA\n",
        "# ---------------------------------\n",
        "prompt_usuario = 'image6.png'\n",
        "respuesta_usuario = generate_response(prompt_usuario)\n",
        "print(f\"\\nüîπ Respuesta:\\n{respuesta_usuario}\")\n",
        "\n",
        "# ---------------------------------\n",
        "# CONFIGURACI√ìN DEL BOT DE TELEGRAM\n",
        "# ---------------------------------\n",
        "load_dotenv()\n",
        "\n",
        "logging.basicConfig(\n",
        "    format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\",\n",
        "    level=logging.INFO,\n",
        "    handlers=[logging.StreamHandler(sys.stdout)]\n",
        ")\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "from telegram import Update\n",
        "from telegram.ext import Application, CommandHandler, MessageHandler, filters, CallbackContext\n",
        "\n",
        "async def start(update: Update, context: CallbackContext) -> None:\n",
        "    await update.message.reply_text(\"¬°Hola! Soy tu especialista en ciberseguridad. ¬øEn qu√© te puedo ayudar?\")\n",
        "\n",
        "async def handle_message(update: Update, context: CallbackContext) -> None:\n",
        "    user_input = update.message.text\n",
        "    username = update.message.from_user.username\n",
        "    logger.info(f\"Mensaje recibido de {username}: {user_input}\")\n",
        "    response = generate_response(user_input)\n",
        "    await update.message.reply_text(response)\n",
        "\n",
        "def main():\n",
        "    TELEGRAM_TOKEN = \"7047664203:AAEa-JEcZQpv-tDCIdV6ZE_odp4lPTH0Bd8\"\n",
        "    if not TELEGRAM_TOKEN:\n",
        "        logger.error(\"El token de Telegram no est√° configurado.\")\n",
        "        sys.exit(1)\n",
        "\n",
        "    app_telegram = Application.builder().token(TELEGRAM_TOKEN).build()\n",
        "    app_telegram.add_handler(CommandHandler(\"start\", start))\n",
        "    app_telegram.add_handler(MessageHandler(filters.TEXT & ~filters.COMMAND, handle_message))\n",
        "    logger.info(\"Bot iniciado y ejecut√°ndose...\")\n",
        "    app_telegram.run_polling()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lt2W1RFiodOV",
        "outputId": "cffc2e82-25df-481b-e971-31b04fff2083"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rHit:1 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "\r0% [Connecting to security.ubuntu.com] [Waiting for headers] [Connected to r2u.stat.illinois.edu (19\r                                                                                                    \rGet:2 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n",
            "\r0% [Waiting for headers] [Connecting to security.ubuntu.com] [2 InRelease 3,632 B/3,632 B 100%] [Wai\r0% [Waiting for headers] [Connecting to security.ubuntu.com] [Waiting for headers] [Waiting for head\r                                                                                                    \rGet:3 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "\r0% [3 InRelease 5,484 B/128 kB 4%] [Connecting to security.ubuntu.com] [Waiting for headers] [Waitin\r                                                                                                    \rGet:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "Get:5 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Get:7 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Get:8 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ Packages [68.9 kB]\n",
            "Get:9 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [1,338 kB]\n",
            "Get:10 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,665 kB]\n",
            "Get:11 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,533 kB]\n",
            "Get:12 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease [18.1 kB]\n",
            "Get:13 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [8,708 kB]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [2,956 kB]\n",
            "Get:15 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease [24.3 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [3,830 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu jammy-backports/universe amd64 Packages [35.2 kB]\n",
            "Hit:18 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:19 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy/main amd64 Packages [34.0 kB]\n",
            "Get:20 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,235 kB]\n",
            "Get:21 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy/main amd64 Packages [57.8 kB]\n",
            "Get:22 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [2,652 kB]\n",
            "Get:23 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [3,694 kB]\n",
            "Fetched 29.2 MB in 2s (13.0 MB/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  tesseract-ocr-eng tesseract-ocr-osd\n",
            "The following NEW packages will be installed:\n",
            "  tesseract-ocr tesseract-ocr-eng tesseract-ocr-osd\n",
            "0 upgraded, 3 newly installed, 0 to remove and 119 not upgraded.\n",
            "Need to get 4,816 kB of archives.\n",
            "After this operation, 15.6 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-eng all 1:4.00~git30-7274cfa-1.1 [1,591 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-osd all 1:4.00~git30-7274cfa-1.1 [2,990 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr amd64 4.1.1-2.1build1 [236 kB]\n",
            "Fetched 4,816 kB in 1s (9,481 kB/s)\n",
            "Selecting previously unselected package tesseract-ocr-eng.\n",
            "(Reading database ... 124788 files and directories currently installed.)\n",
            "Preparing to unpack .../tesseract-ocr-eng_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-eng (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-osd.\n",
            "Preparing to unpack .../tesseract-ocr-osd_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-osd (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr.\n",
            "Preparing to unpack .../tesseract-ocr_4.1.1-2.1build1_amd64.deb ...\n",
            "Unpacking tesseract-ocr (4.1.1-2.1build1) ...\n",
            "Setting up tesseract-ocr-eng (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-osd (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr (4.1.1-2.1build1) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Requirement already satisfied: pytesseract in /usr/local/lib/python3.11/dist-packages (0.3.13)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.11/dist-packages (from pytesseract) (24.2)\n",
            "Requirement already satisfied: Pillow>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from pytesseract) (11.1.0)\n",
            "tesseract 4.1.1\n",
            " leptonica-1.82.0\n",
            "  libgif 5.1.9 : libjpeg 8d (libjpeg-turbo 2.1.1) : libpng 1.6.37 : libtiff 4.3.0 : zlib 1.2.11 : libwebp 1.2.2 : libopenjp2 2.4.0\n",
            " Found AVX512BW\n",
            " Found AVX512F\n",
            " Found AVX2\n",
            " Found AVX\n",
            " Found FMA\n",
            " Found SSE\n",
            " Found libarchive 3.6.0 zlib/1.2.11 liblzma/5.2.5 bz2lib/1.0.8 liblz4/1.9.3 libzstd/1.4.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import torch\n",
        "import requests\n",
        "import chromadb\n",
        "from pyngrok import ngrok\n",
        "from fastapi import FastAPI\n",
        "from google.colab import drive\n",
        "from huggingface_hub import notebook_login\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import uvicorn\n",
        "import threading\n",
        "from datasets import load_dataset, Dataset, DatasetDict\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    TrainingArguments,\n",
        "    BitsAndBytesConfig\n",
        ")\n",
        "from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model\n",
        "from trl import SFTTrainer\n",
        "import random\n",
        "import pytesseract\n",
        "import spacy\n",
        "import re\n",
        "from PIL import Image\n",
        "from google.colab import files\n",
        "import sys\n",
        "import logging\n",
        "from telegram import Update\n",
        "from telegram.ext import Application, CommandHandler, MessageHandler, filters, CallbackContext\n",
        "from dotenv import load_dotenv\n",
        "from PIL import Image, ImageEnhance\n",
        "import re\n",
        "import requests"
      ],
      "metadata": {
        "id": "mvcf9YDH8oqp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# üöÄ Montar Google Drive para cargar JSON\n",
        "drive.mount('/content/drive')\n",
        "BASE_DIR = \"/content/drive/My Drive/Trabajo Final Bootcamp\"\n",
        "NORMATIVA_DIR = os.path.join(BASE_DIR, \"normativa\")\n",
        "EMBEDDINGS_BACKUP_PATH = os.path.join(BASE_DIR, \"embeddings.json\")"
      ],
      "metadata": {
        "id": "EX7hb_HuJZax",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "264e23a3-4755-4ad7-ec79-04a2fcf89078"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Verificar que la carpeta de normativa exista\n",
        "if not os.path.exists(NORMATIVA_DIR):\n",
        "    raise FileNotFoundError(f\"La carpeta {NORMATIVA_DIR} no existe. Verifica la ruta o crea la carpeta.\")"
      ],
      "metadata": {
        "id": "5rF5j2t53jvR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Login en Hugging Face (ejecutar solo una vez)\n",
        "notebook_login()"
      ],
      "metadata": {
        "id": "r3KImBLND22c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17,
          "referenced_widgets": [
            "344f9d047a82415eae5d25520621937b",
            "47ba9942506b4f9f9a067b877decc515",
            "07d96c0bf258491480ab501c27f92dc5",
            "852d14b1d1264880a9846a211a7ef60b",
            "02861aa887e54e768d30d86097b73e96",
            "da88884cc0274209b132fdbaa72fa22a",
            "57a488f4f38d4db48ef87ee8b8988954",
            "5100674e9785441bbe40b8663dafa9db",
            "bc1c9151c396411e98f7efc414ab88e3",
            "75ab0302fea54c05946073fdcab67348",
            "66f304bcf4734f07a169c5209feee002",
            "faa7bf69e91443d8a4dedbd9e1f54b96",
            "8de4edacaa6b436ca2d8ae5c878423b3",
            "7a674adf0ea040458fd4c081c417a110",
            "97130284ce9a4ea48ac2dd0da4c83810",
            "bf7bfd94f9c84038a94658998a5a86d3",
            "55efe188497d426f9afc6b618191ba8a",
            "6836bdc7830e4fc7b0c6e7c6f2281f4a",
            "4922ea9df59443008a73f8cb1f258bfe",
            "1870d1bf5715423a991e960a3b2e77c9"
          ]
        },
        "outputId": "a46982c7-2383-471a-fd19-89f97c615ea7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv‚Ä¶"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "344f9d047a82415eae5d25520621937b"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cargo los modelos"
      ],
      "metadata": {
        "id": "r3n-WUsABkxT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# üöÄ Configurar GPU A100\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"‚úÖ Usando dispositivo: {device}\")\n",
        "\n",
        "# üöÄ Cargar Modelo y Tokenizador\n",
        "model_name = \"CasiAC/deepseek-r1-8b-ciberseguridad\"\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\"\n",
        ")\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name, quantization_config=bnb_config, device_map=\"auto\", trust_remote_code=True)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "embedding_model = SentenceTransformer('all-MiniLM-L6-v2', device=device)\n",
        "print(\"‚úÖ Modelo cargado üöÄ\")"
      ],
      "metadata": {
        "id": "DahgMeIc8nBx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 188,
          "referenced_widgets": [
            "e9f1a86ea76e4d73b787fac08f597db7",
            "5060ef17028249f783f8c961f6df5e6a",
            "423f7d49213241bd9daf4fa7f083cb10",
            "9e52930293164fdd904f3205c20702fb",
            "ed6ff4a017384d848f4ec9e2b8fb4200",
            "b3c5453e3a9942e69b8e5abb6e09937c",
            "7f1d6aa2c1fd4424a2d5bc911ee127aa",
            "413ce24651c849aeb1bce9ba6da909b5",
            "19c071aeac61459c948ba8ce6689460e",
            "cefd59584a764cb08f4f2ecf2795ee2d",
            "638a924f8e09441cbde08de9051d637e"
          ]
        },
        "outputId": "21a022f9-0ab3-4634-a67e-06e290ac827a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Usando dispositivo: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e9f1a86ea76e4d73b787fac08f597db7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Modelo cargado üöÄ\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Configurar ChromaDB en un contenedor Docker"
      ],
      "metadata": {
        "id": "Nlc6y1rZdcvS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Iniciar el servidor FastAPI\n",
        "app = FastAPI()"
      ],
      "metadata": {
        "id": "Y1XzY-eoOKKh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Configuraci√≥n de ngrok para exponer el puerto 8000\n",
        "ngrok.set_auth_token(\"2sN2ljFFRN4UpJk7VPL6jPiHVJL_6FafRKrvugJysTGGRV1KB\")\n",
        "public_url = ngrok.connect(8000)\n",
        "print(f\"üîó ChromaDB API accesible en: {public_url}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VyG0nAwrBY8R",
        "outputId": "e686c6db-ae6a-4e10-8748-fa61c2615628"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîó ChromaDB API accesible en: NgrokTunnel: \"https://74dc-35-188-84-114.ngrok-free.app\" -> \"http://localhost:8000\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Crear cliente y colecci√≥n en ChromaDB ---\n",
        "client = chromadb.Client()"
      ],
      "metadata": {
        "id": "hJDB44B5AicR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Se unifica la variable como \"collection\" para mayor consistencia\n",
        "collection = client.create_collection(name=\"test\", metadata={\"hnsw:search_ef\": 100, \"hnsw:construction_ef\": 1000})"
      ],
      "metadata": {
        "id": "dg_ahHV1Og8t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Definici√≥n de endpoints de FastAPI ---\n",
        "@app.get(\"/\")\n",
        "def read_root():\n",
        "    return {\"message\": \"Chroma API is running!\"}\n",
        "\n",
        "@app.get(\"/collections\")\n",
        "def get_collections():\n",
        "    collections = client.list_collections()\n",
        "    return {\"collections\": collections}\n",
        "\n",
        "@app.get(\"/collections/{collection_name}\")\n",
        "def get_collection(collection_name: str):\n",
        "    coll = client.get_collection(name=collection_name)\n",
        "    return {\"collection\": coll}\n",
        "\n",
        "@app.post(\"/collections/{collection_name}/add\")\n",
        "def add_to_collection(collection_name: str, item: dict):\n",
        "    coll = client.get_collection(name=collection_name)\n",
        "    coll.add(\n",
        "        documents=[item[\"document\"]],\n",
        "        metadatas=[item.get(\"metadata\", {})],\n",
        "        ids=[item.get(\"id\", \"default_id\")]\n",
        "    )\n",
        "    return {\"message\": f\"Item added to collection {collection_name}\"}\n",
        "\n",
        "@app.get(\"/health\")\n",
        "def health_check():\n",
        "    return {\"status\": \"OK\"}"
      ],
      "metadata": {
        "id": "eLzIxdWfEAOx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Iniciar el servidor FastAPI en un hilo de fondo para no bloquear Colab\n",
        "def start_server():\n",
        "    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n",
        "\n",
        "server_thread = threading.Thread(target=start_server, daemon=True)\n",
        "server_thread.start()"
      ],
      "metadata": {
        "id": "lrxWY6dUPEJm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cargar documentos JSON y Generar Embeddings"
      ],
      "metadata": {
        "id": "c4B6cf3-Eh_H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# üöÄ Cargar Documentos y Guardar Embeddings en ChromaDB\n",
        "def cargar_documentos_y_embeddings():\n",
        "    embeddings_dict = {}\n",
        "    for archivo in os.listdir(NORMATIVA_DIR):\n",
        "        ruta_json = os.path.join(NORMATIVA_DIR, archivo)\n",
        "        with open(ruta_json, \"r\", encoding=\"utf-8\") as f:\n",
        "            documentos = json.load(f)\n",
        "\n",
        "        for section in documentos.get(\"sections\", []):\n",
        "            doc_id = f\"{archivo}_p{section['page']}\"\n",
        "            content = section[\"content\"]\n",
        "            embedding = embedding_model.encode(content, convert_to_tensor=True).cpu().numpy().tolist()\n",
        "            embeddings_dict[doc_id] = embedding\n",
        "\n",
        "            # Enviar embedding a ChromaDB\n",
        "            collection.add(\n",
        "                documents=[content],\n",
        "                metadatas=[{\"title\": documentos.get(\"title\", \"Desconocido\"), \"page\": section[\"page\"]}],\n",
        "                embeddings=[embedding],\n",
        "                ids=[doc_id]\n",
        "            )\n",
        "\n",
        "    # Guardar copia en Drive\n",
        "    with open(EMBEDDINGS_BACKUP_PATH, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(embeddings_dict, f, ensure_ascii=False, indent=4)\n",
        "    print(f\"‚úÖ {len(embeddings_dict)} documentos indexados y guardados en ChromaDB üöÄ\")\n",
        "    return embeddings_dict\n",
        "\n",
        "embeddings_dict = cargar_documentos_y_embeddings()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7BlxM3XWV44A",
        "outputId": "72d7a5d7-d7ee-4573-b7fe-48405a8b24a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ 215 documentos indexados y guardados en ChromaDB üöÄ\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implementar el RAG"
      ],
      "metadata": {
        "id": "jvqaJOPtgXsJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# üöÄ Funci√≥n para Obtener Contexto desde ChromaDB\n",
        "def obtener_contexto(pregunta, n_docs=3):\n",
        "    embedding_pregunta = embedding_model.encode([pregunta], convert_to_tensor=True).cpu().numpy().tolist()\n",
        "    resultados = collection.query(\n",
        "        query_embeddings=embedding_pregunta,\n",
        "        n_results=n_docs\n",
        "    )\n",
        "    documents = resultados.get('documents', [])\n",
        "    if not documents:\n",
        "        return \"No se encontraron documentos relevantes.\"\n",
        "\n",
        "    # Convertir cada documento a cadena: si es una lista, unir sus elementos; si no, convertir a string\n",
        "    documentos_convertidos = []\n",
        "    for doc in documents:\n",
        "        if isinstance(doc, list):\n",
        "            documentos_convertidos.append(\" \".join(map(str, doc)))\n",
        "        else:\n",
        "            documentos_convertidos.append(str(doc))\n",
        "\n",
        "    return \"\\n\".join(documentos_convertidos)\n"
      ],
      "metadata": {
        "id": "WWGkxSa1f-_p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# üöÄ Funci√≥n para Generar Respuesta\n",
        "def generar_respuesta(pregunta, max_tokens=300, temperatura=0.1):\n",
        "    contexto = obtener_contexto(pregunta)\n",
        "    entrada = f\"Contexto: {contexto}\\nPregunta: {pregunta}\\nRespuesta:\"\n",
        "    inputs = tokenizer(entrada, return_tensors=\"pt\").to(device)\n",
        "    output = model.generate(**inputs, max_new_tokens=max_tokens, temperature=temperatura, top_p=0.9, repetition_penalty=1.05)\n",
        "    respuesta = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "    return respuesta\n"
      ],
      "metadata": {
        "id": "t4MuhUWCL5S3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# üöÄ Probar el modelo con el RAG\n",
        "pregunta = \"¬øC√≥mo mejorar la seguridad en mi empresa?\"\n",
        "respuesta = generar_respuesta(pregunta)\n",
        "print(\"üõ°Ô∏è Respuesta Generada:\")\n",
        "print(respuesta)"
      ],
      "metadata": {
        "id": "nGoZKp9ef-xj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "935fffd6-986c-453f-9fcb-5867882fc019"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üõ°Ô∏è Respuesta Generada:\n",
            "Contexto: ISO 27001:2022 IMPLEMENTATION GUIDE\n",
            "14\n",
            "SECCI√ìN 4: \n",
            "CONTEXTO DE LA \n",
            "ORGANIZACI√ìN\n",
            "Contexto interno\n",
            "Los siguientes son ejemplos de las √°reas que pueden tenerse \n",
            "en cuenta al evaluar las cuestiones internas que pueden influir \n",
            "en los riesgos del SGSI:\n",
            "‚Ä¢ \u0007 Madurez: ¬øEs una empresa √°gil con un lienzo en blanco en \n",
            "el que trabajar, o una instituci√≥n con procesos y controles \n",
            "de seguridad establecidos?\n",
            "‚Ä¢\u0007  Cultura organizativa: ¬øEs su organizaci√≥n relajada en \n",
            "cuanto a c√≥mo, cu√°ndo y d√≥nde trabaja la gente, o \n",
            "extremadamente reglamentada? \n",
            "‚Ä¢ \u0007 Gesti√≥n: ¬øExisten canales y procesos de comunicaci√≥n \n",
            "claros entre los principales responsables de la toma de \n",
            "decisiones y el resto de la organizaci√≥n?\n",
            "‚Ä¢\u0007  Tama√±o de los recursos: ¬øTrabaja con un equipo de \n",
            "seguridad de la informaci√≥n o lo hace todo una persona?\n",
            "‚Ä¢\u0007  Madurez de los recursos: ¬øLos recursos disponibles est√°n \n",
            "informados, plenamente formados, son fiables y constantes, \n",
            "o el personal carece de experiencia y cambia \n",
            "constantemente?\n",
            "‚Ä¢\u0007  Formatos de los activos de informaci√≥n: ¬øSus activos de \n",
            "informaci√≥n se almacenan principalmente en formato \n",
            "impreso o electr√≥nicamente en un servidor o en sistemas \n",
            "remotos basados en la nube?\n",
            "‚Ä¢\u0007  Sensibilidad/valor de los activos de informaci√≥n: ¬øSu \n",
            "organizaci√≥n tiene que gestionar activos de informaci√≥n \n",
            "muy valiosos?\n",
            "‚Ä¢ \u0007 Coherencia: ¬øDispone de procesos uniformes en toda la \n",
            "organizaci√≥n o de una multitud de pr√°cticas operativas \n",
            "diferentes con poca coherencia?\n",
            "‚Ä¢ \u0007 Sistemas: ¬øTiene su organizaci√≥n muchos sistemas que \n",
            "funcionan con versiones de software que ya no son \n",
            "compatibles con el fabricante, o mantiene la tecnolog√≠a m√°s \n",
            "actualizada y mejor disponible?\u0007\n",
            "‚Ä¢\u0007  Complejidad del sistema: ¬øUtiliza un sistema principal que \n",
            "hace todo el trabajo pesado, o varios sistemas \n",
            "departamentales con una transferencia de informaci√≥n \n",
            "limitada entre ellos?\n",
            "‚Ä¢\u0007  Espacio f√≠sico: ¬øDisponen de una oficina propia y segura, o \n",
            "trabajan en un espacio compartido con otras \n",
            "organizaciones, o son una organizaci√≥n exclusivamente \n",
            "remota?\n",
            "Contexto externo\n",
            "Los siguientes son ejemplos de las √°reas que pueden \n",
            "tenerse en cuenta al evaluar las cuestiones externas que \n",
            "pueden influir en los riesgos del SGSI:\n",
            "‚Ä¢ \u0007 La competencia: ¬øOpera en un mercado innovador y en \n",
            "evoluci√≥n, que requiere actualizaciones de los sistemas \n",
            "para seguir siendo competitivo, o en un mercado maduro y \n",
            "estable con pocas innovaciones?\n",
            "‚Ä¢ \u0007 Propietario: ¬øNecesita aprobaci√≥n para mejorar la \n",
            "seguridad f√≠sica?\n",
            "‚Ä¢\u0007  Reguladores: ¬øExiste en su sector la obligaci√≥n de realizar \n",
            "cambios reglamentarios con regularidad, o hay poca \n",
            "supervisi√≥n por parte de los organismos reguladores?\n",
            "‚Ä¢ \u0007 Econ√≥mico/pol√≠tico: ¬øInfluyen las fluctuaciones monetarias \n",
            "en su organizaci√≥n? ¬øC√≥mo afectan las situaciones \n",
            "geopol√≠ticas a su organizaci√≥n?\n",
            "‚Ä¢\u0007  Consideraciones ambientales: ¬øEst√°n sus instalaciones en \n",
            "una llanura inundable y los servidores en un s√≥tano? \n",
            "¬øExisten factores que hagan de sus instalaciones un \n",
            "posible objetivo de robo o atentado terrorista (por ejemplo, \n",
            "en un lugar c√©ntrico o cerca de un posible objetivo)?\n",
            "‚Ä¢\u0007  Prevalencia de los ataques a la seguridad de la \n",
            "informaci√≥n: ¬øSu organizaci√≥n opera en un sector que sufre \n",
            "ciberataques?\n",
            "‚Ä¢ \u0007 Accionistas: ¬øEst√°n muy preocupados por la vulnerabilidad \n",
            "de la organizaci√≥n a las violaciones de datos? ¬øHasta qu√© \n",
            "punto les preocupa el coste de los esfuerzos de la \n",
            "organizaci√≥n por mejorar su seguridad de la informaci√≥n?\n",
            "El prop√≥sito de su SGSI es proteger los Activos de Informaci√≥n de su organizaci√≥n, para \n",
            "que pueda alcanzar sus objetivos.\n",
            "La forma de hacerlo y las √°reas espec√≠ficas de prioridad \n",
            "depender√°n del contexto en el que opere su organizaci√≥n.  \n",
            "organizaci√≥n:\n",
            "‚Ä¢ \u0007 Internamente: las cosas sobre las que la \n",
            "organizaci√≥n tiene cierto control.\n",
            "‚Ä¢ \u0007 Externamente: las cosas que la organizaci√≥n no \n",
            "controla directamente.\n",
            "Un an√°lisis cuidadoso del entorno en el que opera su \n",
            "organizaci√≥n es fundamental para identificar los \n",
            "riesgos inherentes a la seguridad de sus activos de \n",
            "informaci√≥n. El an√°lisis es la base que le permitir√° \n",
            "evaluar qu√© procesos debe considerar a√±adir o \n",
            "reforzar para construir un SGSI eficaz. ISO 27001:2022 IMPLEMENTATION GUIDE\n",
            "16\n",
            "SECCI√ìN 5:  \n",
            "LIDERAZGO\n",
            "Pol√≠tica de seguridad info.\n",
            "Una responsabilidad vital de la direcci√≥n es establecer y \n",
            "documentar una Pol√≠tica de Seguridad de la Informaci√≥n (PSI) \n",
            "que est√© alineada con los objetivos clave de la organizaci√≥n. \n",
            "Debe incluir objetivos o un marco para establecerlos. Para \n",
            "demostrar que est√° alineada con el contexto de la \n",
            "organizaci√≥n y los requisitos de las principales partes \n",
            "interesadas, se recomienda que haga referencia o contenga \n",
            "un resumen de los principales problemas y requisitos que \n",
            "debe gestionar. Tambi√©n debe incluir el compromiso de:\n",
            "‚Ä¢\u0007  Cumplir los requisitos aplicables en materia de seguridad de \n",
            "la informaci√≥n, como los requisitos legales, las expectativas \n",
            "de los clientes y los compromisos contractuales.\n",
            "‚Ä¢\u0007  La mejora continua de su SGSI.\n",
            "El PSI puede hacer referencia a, o incluir sub-pol√≠ticas que \n",
            "cubran, los controles clave del SGSI de la organizaci√≥n. \n",
            "Algunos ejemplos son: la selecci√≥n de proveedores cr√≠ticos \n",
            "para la seguridad de la informaci√≥n, la contrataci√≥n y \n",
            "formaci√≥n de los empleados, clear desk y clear screen, \n",
            "controles criptogr√°ficos, controles de acceso, etc.\n",
            "Para demostrar la importancia del PSI, es aconsejable que lo \n",
            "autorice el miembro de mayor rango de su Alta Direcci√≥n o \n",
            "cada uno de los miembros del equipo de Alta Direcci√≥n.\n",
            "CONSEJO: Para asegurarse de que su PSI est√° bien \n",
            "comunicado y a disposici√≥n de las partes interesadas, \n",
            "recomendamos:\n",
            "‚Ä¢\u0007  IIncl√∫yala en los paquetes de iniciaci√≥n y en las \n",
            "presentaciones para nuevos empleados y contratistas.\n",
            "‚Ä¢\u0007  Publique la declaraci√≥n clave en los tablones de anuncios \n",
            "internos, las intranets y el sitio web de su organizaci√≥n.\n",
            "‚Ä¢\u0007  Haga que su cumplimiento y/o apoyo sea un requisito \n",
            "contractual para empleados, contratistas y proveedores \n",
            "cr√≠ticos para la seguridad de la informaci√≥n.\n",
            "Funciones y responsabilidades\n",
            "Para que las actividades de seguridad de la informaci√≥n \n",
            "formen parte de las actividades de la mayor√≠a de las personas \n",
            "de la organizaci√≥n, las responsabilidades y las obligaciones \n",
            "de rendir cuentas deben definirse y comunicarse claramente.\n",
            "Aunque la norma no exige la designaci√≥n de un \n",
            "representante de seguridad de la informaci√≥n, puede ser √∫til \n",
            "para algunas organizaciones nombrar a uno que dirija un \n",
            "equipo de seguridad de la informaci√≥n para coordinar la \n",
            "formaci√≥n, supervisar los controles e informar sobre el \n",
            "funcionamiento del SGSI a la alta direcci√≥n.  Es posible que \n",
            "esta persona ya sea responsable de la protecci√≥n de datos.\n",
            "Sin embargo, para desempe√±ar su funci√≥n con eficacia, lo \n",
            "ideal es que forme parte del equipo de alta direcci√≥n y que \n",
            "tenga s√≥lidos conocimientos t√©cnicos sobre gesti√≥n de la \n",
            "seguridad de la informaci√≥n.\n",
            "Evidenciar liderazgo al auditor \n",
            "La Direcci√≥n ser√°n aquellos que establecen la direcci√≥n \n",
            "estrat√©gica y aprueban la asignaci√≥n de recursos para la \n",
            "organizaci√≥n o √°rea de negocio con el alcance de su SGSI. \n",
            "Dependiendo de c√≥mo est√© estructurada su organizaci√≥n, \n",
            "estas personas pueden ser el equipo directivo diario. Un \n",
            "auditor normalmente pondr√° a prueba el liderazgo mediante \n",
            "una entrevista, y evaluar√° su nivel de implicaci√≥n en el:\n",
            "‚Ä¢\u0007  Evaloraci√≥n de riesgos y oportunidades.\n",
            "‚Ä¢\u0007  Establecimiento y comunicaci√≥n de pol√≠ticas.\n",
            "‚Ä¢\u0007  Fijaci√≥n y comunicaci√≥n de objetivos.\n",
            "‚Ä¢\u0007  Revisi√≥n y comunicaci√≥n del rendimiento del sistema.\n",
            "‚Ä¢\u0007  Asignaci√≥n de recursos, responsabilidades y obligaciones \n",
            "adecuadas.\n",
            "CONSEJO: Antes de su auditor√≠a externa, identifique qui√©n \n",
            "de la alta direcci√≥n se reunir√° con el auditor externo. \n",
            "Prep√°relos con un simulacro de entrevista que incluya las \n",
            "preguntas que espera que les hagan.\n",
            "El liderazgo en este contexto significa la \n",
            "participaci√≥n en el establecimiento de la \n",
            "direcci√≥n del SGSI, su aplicaci√≥n y \n",
            "provisi√≥n de recursos. Esto incluye:\n",
            "‚Ä¢\u0007  Garantizar que los objetivos del \n",
            "SGSI sean claros y est√©n alineados \n",
            "con la estrategia general.\n",
            "‚Ä¢\u0007  Claridad en las responsabilidades y \n",
            "la rendici√≥n de cuentas.\n",
            "‚Ä¢\u0007  El pensamiento basado en el riesgo \n",
            "est√° en el centro de toda toma de \n",
            "decisiones\n",
            "‚Ä¢\u0007  Comunicaci√≥n clara de esta \n",
            "informaci√≥n a todas las personas \n",
            "dentro del √°mbito de su SGSI.\n",
            "La norma ISO 27001 concede gran \n",
            "importancia al compromiso activo de \n",
            "la Direcci√≥n en el SGSI, partiendo de \n",
            "la base de que el compromiso de la \n",
            "Direcci√≥n es crucial para garantizar la \n",
            "implantaci√≥n efectiva y el \n",
            "mantenimiento de un SGSI eficaz por \n",
            "parte de los empleados.\n",
            "La importancia del liderazgo d) cuando el encargado del tratamiento no sea una instituci√≥n u organismo de la Uni√≥n, normas corporativas vinculantes, \n",
            "c√≥digos de conducta o mecanismos de certificaci√≥n con arreglo al art√≠culo 46, apartado 2, letras b), e) y f), del \n",
            "Reglamento (UE) 2016/679.\n",
            "3.\n",
            "Siempre que exista autorizaci√≥n del Supervisor Europeo de Protecci√≥n de Datos, las garant√≠as adecuadas referidas en el \n",
            "apartado 1 tambi√©n podr√°n ser aportadas, en particular, mediante:\n",
            "a) cl√°usulas contractuales entre el responsable o el encargado y el responsable, encargado o destinatario de los datos \n",
            "personales en el tercer pa√≠s u organizaci√≥n internacional, o\n",
            "b) disposiciones que se incorporen en acuerdos administrativos entre las autoridades u organismos p√∫blicos que incluyan \n",
            "derechos efectivos y exigibles para los interesados.\n",
            "4.\n",
            "Las autorizaciones concedidas por el Supervisor Europeo de Protecci√≥n de Datos de conformidad con el art√≠culo 9, \n",
            "apartado 7, del Reglamento (CE) n.o 45/2001 seguir√°n siendo v√°lidas hasta que hayan sido modificadas, sustituidas o \n",
            "derogadas, en caso necesario, por este.\n",
            "5.\n",
            "Las instituciones y organismos de la Uni√≥n informar√°n al Supervisor Europeo de Protecci√≥n de Datos de las categor√≠as \n",
            "de casos en que el presente art√≠culo haya sido aplicado.\n",
            "Art√≠culo 49\n",
            "Transferencias o comunicaciones no autorizadas por el Derecho de la Uni√≥n\n",
            "Cualquier sentencia de un √≥rgano jurisdiccional o decisi√≥n de una autoridad administrativa de un tercer pa√≠s que exijan que \n",
            "un responsable o encargado del tratamiento transfiera o comunique datos personales √∫nicamente ser√° reconocida o \n",
            "ejecutable en cualquier modo si se basa en un acuerdo internacional, como un tratado de asistencia jur√≠dica mutua, vigente \n",
            "entre el tercer pa√≠s requirente y la Uni√≥n, sin perjuicio de otros motivos para la transferencia al amparo del presente \n",
            "cap√≠tulo.\n",
            "Art√≠culo 50\n",
            "Excepciones para situaciones espec√≠ficas\n",
            "1.\n",
            "A falta de una decisi√≥n de adecuaci√≥n de conformidad con el art√≠culo 45, apartado 3, del Reglamento (UE) 2016/679, \n",
            "o el art√≠culo 36, apartado 3, de la Directiva (UE) 2016/680, o de garant√≠as adecuadas de conformidad con el art√≠culo 48 del \n",
            "presente Reglamento, solo podr√° realizarse una transferencia o una serie de transferencias de datos personales a un tercer \n",
            "pa√≠s o una organizaci√≥n internacional si se cumple alguna de las condiciones siguientes:\n",
            "a) el interesado haya prestado expl√≠citamente su consentimiento a la transferencia propuesta, tras haber sido informado de \n",
            "los posibles riesgos para √©l de dichas transferencias debido a la ausencia de una decisi√≥n de adecuaci√≥n y de garant√≠as \n",
            "adecuadas;\n",
            "b) la transferencia sea necesaria para el cumplimiento de un contrato entre el interesado y el responsable del tratamiento o \n",
            "para la ejecuci√≥n de medidas precontractuales adoptadas a solicitud del interesado;\n",
            "c) la transferencia sea necesaria para la celebraci√≥n o el cumplimiento de un contrato, en inter√©s del interesado, entre el \n",
            "responsable del tratamiento y otra persona f√≠sica o jur√≠dica;\n",
            "d) la transferencia sea necesaria por razones importantes de inter√©s p√∫blico;\n",
            "e) la transferencia sea necesaria para la formulaci√≥n, el ejercicio o la defensa de reclamaciones; o\n",
            "f) la transferencia sea necesaria para proteger los intereses vitales del interesado o de otras personas, cuando el interesado \n",
            "est√© f√≠sica o jur√≠dicamente incapacitado para prestar su consentimiento; o\n",
            "g) la transferencia se realice desde un registro que, con arreglo al Derecho de la Uni√≥n, tenga por objeto proporcionar \n",
            "informaci√≥n al p√∫blico y que est√© disponible para consulta del p√∫blico en general o de cualquier persona que pueda \n",
            "demostrar un inter√©s leg√≠timo, pero solo en la medida en que en ese caso particular se cumplan las condiciones que \n",
            "establece el Derecho de la Uni√≥n para la consulta.\n",
            "2.\n",
            "Las letras a), b) y c) del apartado 1 no ser√°n aplicables a las actividades llevadas a cabo por las instituciones y \n",
            "organismos de la Uni√≥n en el ejercicio de sus potestades p√∫blicas.\n",
            "3.\n",
            "El inter√©s p√∫blico indicado en el apartado 1, letra d), ser√° reconocido por el Derecho de la Uni√≥n.\n",
            "4.\n",
            "Una transferencia efectuada de conformidad con el apartado 1, letra g), no abarcar√° la totalidad de los datos \n",
            "personales ni categor√≠as enteras de datos personales contenidos en el registro, a menos que as√≠ lo autorice el Derecho de la \n",
            "Uni√≥n. Si la finalidad del registro es la consulta por parte de personas que tengan un inter√©s leg√≠timo, la transferencia solo se \n",
            "efectuar√° a solicitud de dichas personas o si estas han de ser las destinatarias.\n",
            "21.11.2018\n",
            "ES\n",
            "Diario Oficial de la Uni√≥n Europea\n",
            "L 295/79\n",
            "Pregunta: ¬øC√≥mo mejorar la seguridad en mi empresa?\n",
            "Respuesta: Aprovecha de la gu√≠a de implementaci√≥n de ISO 27001:2022 para identificar los riesgos y fortalecer tu SGSI.\n",
            "\n",
            "Contexto interno\n",
            "Los siguientes son ejemplos de las √°reas que pueden tenerse en cuenta al evaluar las cuestiones internas que pueden influir en los riesgos del SGSI:\n",
            "‚Ä¢  Madurez: ¬øEs una empresa √°gil con un lienzo en blanco en el que trabajar, o una instituci√≥n con procesos y controles de seguridad establecidos?\n",
            "‚Ä¢  Cultura organizativa: ¬øEs su organizaci√≥n relajada en cuanto a c√≥mo, cu√°ndo y d√≥nde trabaja la gente, o extremadamente reglamentada?\n",
            "‚Ä¢  Gesti√≥n: ¬øExisten canales y procesos de comunicaci√≥n claros entre los principales responsables de la toma de decisiones y el resto de la organizaci√≥n?\n",
            "‚Ä¢  Tama√±o de los recursos: ¬øTrabaja con un equipo de seguridad de la informaci√≥n o lo hace todo una persona?\n",
            "‚Ä¢  Madurez de los recursos: ¬øLos recursos disponibles est√°n informados, plenamente formados, son fiables y constantes, o el personal carece de experiencia y cambia constantemente?\n",
            "‚Ä¢  Formatos de los activos de informaci√≥n: ¬øSus activos de informaci√≥n se almacenan principalmente en formato impreso o electr√≥nicamente en un servidor o en sistemas remotos basados en\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Configuraci√≥n Tesseract OCR\n",
        "pytesseract.pytesseract.tesseract_cmd = '/usr/bin/tesseract'\n",
        "nlp = spacy.load(\"en_core_web_sm\")"
      ],
      "metadata": {
        "id": "x1UQEEwVMOPu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#Pipeline modelo + OCR\n",
        "\n",
        "# üîπ API Key de VirusTotal\n",
        "API_KEY = \"06858db9f480b4aba21a5831457a9b919b1f9014e6f8872ee1f4f7d1a029197c\"\n",
        "HEADERS = {\"x-apikey\": API_KEY}\n",
        "\n",
        "def preprocesar_imagen(imagen):\n",
        "    \"\"\"Convierte la imagen a escala de grises y mejora el contraste.\"\"\"\n",
        "    try:\n",
        "        image = Image.open(imagen)\n",
        "        image = image.convert(\"L\")\n",
        "        enhancer = ImageEnhance.Contrast(image)\n",
        "        image = enhancer.enhance(2.0)\n",
        "        return image\n",
        "    except Exception as e:\n",
        "        return None\n",
        "\n",
        "def extraer_texto_img(imagen):\n",
        "    \"\"\"Extrae texto de una imagen tras preprocesarla.\"\"\"\n",
        "    try:\n",
        "        image = preprocesar_imagen(imagen)\n",
        "        if image is None:\n",
        "            return \"Error al procesar la imagen\"\n",
        "        # Extraer texto con pytesseract\n",
        "        texto_extraido = pytesseract.image_to_string(image)\n",
        "        return limpiar_texto(texto_extraido)\n",
        "    except Exception as e:\n",
        "        return f\"Error al procesar la imagen: {str(e)}\"\n",
        "\n",
        "def consultar_ip(ip):\n",
        "    \"\"\"Consulta una IP en VirusTotal y eval√∫a si es segura o maliciosa.\"\"\"\n",
        "    url = f\"https://www.virustotal.com/api/v3/ip_addresses/{ip}\"\n",
        "    response = requests.get(url, headers=HEADERS)\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        data = response.json()\n",
        "        stats = data[\"data\"][\"attributes\"][\"last_analysis_stats\"]\n",
        "        malicious = stats.get(\"malicious\", 0)\n",
        "        harmless = stats.get(\"harmless\", 0)\n",
        "\n",
        "        if malicious > 0:\n",
        "            veredicto = f\"‚ùå La IP {ip} ha sido reportada como **maliciosa** en {malicious} an√°lisis.\"\n",
        "        else:\n",
        "            veredicto = f\"‚úÖ La IP {ip} parece **segura**, sin reportes de actividad maliciosa.\"\n",
        "\n",
        "        return {\n",
        "            \"IP\": ip,\n",
        "            \"Veredicto\": veredicto,\n",
        "            \"An√°lisis\": stats\n",
        "        }\n",
        "    return {\"error\": f\"Error en la consulta: {response.status_code}\"}\n",
        "\n",
        "def consultar_url(url):\n",
        "    \"\"\"Consulta una URL en VirusTotal y eval√∫a si es segura o maliciosa.\"\"\"\n",
        "    scan_url = \"https://www.virustotal.com/api/v3/urls\"\n",
        "    response = requests.post(scan_url, headers=HEADERS, data={\"url\": url})\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        analysis_id = response.json()[\"data\"][\"id\"]\n",
        "        result_url = f\"https://www.virustotal.com/api/v3/analyses/{analysis_id}\"\n",
        "        result_response = requests.get(result_url, headers=HEADERS)\n",
        "\n",
        "        if result_response.status_code == 200:\n",
        "            data = result_response.json()\n",
        "            stats = data[\"data\"][\"attributes\"][\"stats\"]\n",
        "            malicious = stats.get(\"malicious\", 0)\n",
        "            harmless = stats.get(\"harmless\", 0)\n",
        "\n",
        "            if malicious > 0:\n",
        "                veredicto = f\"‚ùå La URL {url} ha sido **marcada como maliciosa** en {malicious} an√°lisis.\"\n",
        "            else:\n",
        "                veredicto = f\"‚úÖ La URL {url} parece **segura**, sin reportes de actividad maliciosa.\"\n",
        "\n",
        "            return {\n",
        "                \"URL\": url,\n",
        "                \"Veredicto\": veredicto,\n",
        "                \"An√°lisis\": stats\n",
        "            }\n",
        "\n",
        "    return {\"error\": f\"Error en la consulta: {response.status_code}\"}\n",
        "\n",
        "def analizar_prompt(prompt):\n",
        "    \"\"\"Detecta si el prompt contiene una IP o URL y consulta VirusTotal si es necesario.\"\"\"\n",
        "\n",
        "    if isinstance(prompt, str) and (prompt.lower().endswith(('.png', '.jpg', '.jpeg', '.gif', '.bmp', '.webp', '.heic'))):\n",
        "        print(f\"üîç Detectada imagen: {prompt}\")\n",
        "        texto_extraido = extraer_texto_img(prompt)\n",
        "        print(f\"Texto extra√≠do de la imagen: {texto_extraido}\")\n",
        "\n",
        "        return analizar_con_modelo(texto_extraido)\n",
        "\n",
        "    ip_pattern = r\"\\b(?:\\d{1,3}\\.){3}\\d{1,3}\\b\"\n",
        "    url_pattern = r'(https?://[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}(?:/[^ \\n]*)?|www\\.[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}(?:/[^ \\n]*)?|[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}(?:/[^ \\n]*)?)'\n",
        "\n",
        "    ip_match = re.search(ip_pattern, prompt)\n",
        "    url_match = re.search(url_pattern, prompt)\n",
        "\n",
        "    if ip_match:\n",
        "        ip = ip_match.group()\n",
        "        print(f\"üîç Detectada IP en el prompt: {ip}\")\n",
        "        return consultar_ip(ip)\n",
        "\n",
        "    if url_match:\n",
        "        url = url_match.group()\n",
        "        print(f\"üîç Detectada URL en el prompt: {url}\")\n",
        "        return consultar_url(url)\n",
        "\n",
        "    return None  # No se detect√≥ ninguna IP o URL\n",
        "\n",
        "def generar_respuesta(prompt):\n",
        "    \"\"\"Genera una respuesta con el modelo o consulta VirusTotal si es necesario.\"\"\"\n",
        "\n",
        "    resultado_api = analizar_prompt(prompt)\n",
        "\n",
        "    if resultado_api:\n",
        "        return json.dumps(resultado_api, indent=4, ensure_ascii=False)  # Respuesta en JSON\n",
        "\n",
        "    # Tokenizar el prompt y generar la respuesta\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
        "\n",
        "    outputs = model.generate(\n",
        "        inputs.input_ids,\n",
        "        attention_mask=inputs.attention_mask,\n",
        "        max_new_tokens=300,\n",
        "        temperature=0.3,\n",
        "        do_sample=True,\n",
        "        top_k=50,\n",
        "        top_p=0.9,\n",
        "        repetition_penalty=1.5,\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        "    )\n",
        "\n",
        "    # Decodificar y limpiar la salida\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True).strip()\n",
        "    return response.replace(prompt, \"\").strip()\n",
        "\n",
        "def analizar_con_modelo(texto_extraido):\n",
        "    \"\"\"Pasa el texto al modelo para que lo analice en busca de se√±ales claras de phishing.\"\"\"\n",
        "\n",
        "    texto_limpio = limpiar_texto(texto_extraido)\n",
        "\n",
        "    system_prompt = \"\"\"\n",
        "      You are a highly specialized AI in cybersecurity. Your primary task is to analyze messages and detect phishing attempts with precision.\n",
        "\n",
        "      üîπ STRICT RULES:\n",
        "      1. Do not invent information. You must base your response ONLY on the given text.\n",
        "      2. Do not provide general explanations unless explicitly asked.\n",
        "      3. Be concise and precise. Your response must be short and strictly relevant.\n",
        "      4. Use formal cybersecurity language and avoid assumptions.\n",
        "      5. Output must always start with either \"Phishing\" or \"Not Phishing\", followed by a brief explanation of why.\n",
        "\n",
        "      üîπ HOW TO ANALYZE A MESSAGE FOR PHISHING:\n",
        "      - Urgency: Does the message pressure the user to act quickly?\n",
        "      - Suspicious Links: Does it contain shortened or untrusted links?\n",
        "      - Requests for Personal Information: Is the user asked to provide passwords or sensitive data?\n",
        "      - Errors or Inconsistencies: Are there grammar mistakes, unnatural tone, or unusual sender details?\n",
        "\n",
        "      üîπ RESPONSE FORMAT (STRICT):\n",
        "      ```\n",
        "      Phishing or Not Phishing\n",
        "      Explanation: [Concise justification, mentioning phishing indicators if present]\n",
        "      ```\n",
        "      ---\n",
        "    \"\"\"\n",
        "\n",
        "    prompt_modelo = f\"\"\"\n",
        "  \t  Analyze the following message and determine whether it is a **phishing attempt** based on the criteria defined in the system instructions.\n",
        "\n",
        "      REMEMBER: Your response must strictly follow the required format. Do not repeat instructions or add unnecessary details.\n",
        "\n",
        "      MESSAGE TO EVALUATE:\n",
        "      {texto_extraido}\n",
        "    \"\"\"\n",
        "\n",
        "    # Tokenizamos el prompt y lo pasamos al modelo\n",
        "    inputs = tokenizer(system_prompt + prompt_modelo, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
        "\n",
        "    outputs = model.generate(\n",
        "        inputs.input_ids,\n",
        "        attention_mask=inputs.attention_mask,\n",
        "        max_new_tokens=300,  # Limitar los tokens de la respuesta\n",
        "        temperature=0.15,  # Controlar la aleatoriedad\n",
        "        do_sample=True,  # Sin aleatorizaci√≥n\n",
        "        top_k=10,  # Menos diversidad\n",
        "        top_p=0.7,\n",
        "        repetition_penalty=1.2,\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        "    )\n",
        "\n",
        "    # Decodificar y limpiar la salida\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True).strip()\n",
        "\n",
        "    if \"</think>\" in response:\n",
        "        user_answer = response.split(\"</think>\", 1)[-1].strip()\n",
        "\n",
        "    return user_answer\n",
        "\n",
        "\n",
        "\n",
        "def limpiar_texto(texto):\n",
        "    \"\"\"Limpia el texto extra√≠do de caracteres no deseados.\"\"\"\n",
        "    texto = texto.strip()\n",
        "    texto = re.sub(r'\\s+', ' ', texto)  # Reemplazar espacios extra\n",
        "    texto = texto.replace(\"\\n\", \" \").replace(\"\\r\", \"\")  # Eliminar saltos de l√≠nea\n",
        "    return texto\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "prompt_usuario_4 = 'image6.png'\n",
        "respuesta_4 = generar_respuesta(prompt_usuario_4)\n",
        "print(f\"\\nüîπ Respuesta para imagen:\\n{respuesta_4}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "GVoVeJpjhE0h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "382a9d81-aece-4ec6-d083-879650727675"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîç Detectada imagen: image6.png\n",
            "Texto extra√≠do de la imagen: Sent on: Friday, June 23, 2023 11:31:12 AM To: Subject: YOUR ACCOUNT IS AT RISK!! Dear Valued User , We received a request from you to terminate your Office 365 email due to a dual college/universities account. This process has begun by our administrator. If you did not authorize this action and you have no knowledge of it, you are advised to re-verify your account. Please give us 24 hours to terminate your account if you initiated the request. Failure to re-verify will result in the closure of your account and you will lose all of my files on these 365 accounts. If this request was made accidentally and you have no knowledge of it, you are advised to copy and paste the URL Below into the address bar of your web browser to fill in the form. cutt.ly/OwtNi6KO Failure to Verify will result in the closure of your account. lowa State University IT Helpdesk All Right Reserved.\n",
            "\n",
            "üîπ Respuesta para imagen:\n",
            "\"Not Phishing  \\nExplanation: The message does not exhibit urgency, suspicious links, requests for personal information, errors, or inconsistencies that would indicate a phishing attempt.\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Funci√≥n para generar respuestas\n",
        "def generate_response(text):\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
        "    output = model.generate(**inputs, max_new_tokens=200, temperature=0.2, do_sample=True)\n",
        "    return tokenizer.decode(output[0], skip_special_tokens=True)"
      ],
      "metadata": {
        "id": "v1nHM3I1d8l5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nest_asyncio\n",
        "nest_asyncio.apply()"
      ],
      "metadata": {
        "id": "6hUB9XOPrNkc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cargar variables de entorno\n",
        "load_dotenv()\n",
        "\n",
        "# Configurar logging\n",
        "logging.basicConfig(\n",
        "    format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\",\n",
        "    level=logging.INFO,\n",
        "    handlers=[logging.StreamHandler(sys.stdout)]\n",
        ")\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Handlers de Telegram\n",
        "async def start(update: Update, context: CallbackContext) -> None:\n",
        "    await update.message.reply_text(\"¬°Hola! Soy tu especialista en ciberseguridad. ¬øEn qu√© te puedo ayudar?\")\n",
        "\n",
        "async def handle_message(update: Update, context: CallbackContext) -> None:\n",
        "    user_input = update.message.text\n",
        "    username = update.message.from_user.username\n",
        "    logger.info(f\"Mensaje recibido de {username}: {user_input}\")\n",
        "\n",
        "    response = generate_response(user_input)  # Usa la funci√≥n importada\n",
        "    await update.message.reply_text(response)\n",
        "\n",
        "# Funci√≥n principal\n",
        "def main():\n",
        "    TELEGRAM_TOKEN = \"7047664203:AAEa-JEcZQpv-tDCIdV6ZE_odp4lPTH0Bd8\"\n",
        "    if not TELEGRAM_TOKEN:\n",
        "        logger.error(\"El token de Telegram no est√° configurado.\")\n",
        "        sys.exit(1)\n",
        "\n",
        "    app = Application.builder().token(TELEGRAM_TOKEN).build()\n",
        "    app.add_handler(CommandHandler(\"start\", start))\n",
        "    app.add_handler(MessageHandler(filters.TEXT & ~filters.COMMAND, handle_message))\n",
        "\n",
        "    logger.info(\"Bot iniciado y ejecut√°ndose...\")\n",
        "    app.run_polling()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "5VZRU4Htd88x",
        "outputId": "a1ed7efc-fd94-425c-eb1a-e470124c119c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Cannot close a running event loop",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-b9bebe5f3a3c>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-29-b9bebe5f3a3c>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Bot iniciado y ejecut√°ndose...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m     \u001b[0mapp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_polling\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/telegram/ext/_application.py\u001b[0m in \u001b[0;36mrun_polling\u001b[0;34m(self, poll_interval, timeout, bootstrap_retries, read_timeout, write_timeout, connect_timeout, pool_timeout, allowed_updates, drop_pending_updates, close_loop, stop_signals)\u001b[0m\n\u001b[1;32m    871\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_task\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m         return self.__run(\n\u001b[0m\u001b[1;32m    874\u001b[0m             updater_coroutine=self.updater.start_polling(\n\u001b[1;32m    875\u001b[0m                 \u001b[0mpoll_interval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpoll_interval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/telegram/ext/_application.py\u001b[0m in \u001b[0;36m__run\u001b[0;34m(self, updater_coroutine, stop_signals, bootstrap_retries, close_loop)\u001b[0m\n\u001b[1;32m   1115\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1116\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mclose_loop\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1117\u001b[0;31m                     \u001b[0mloop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m     def create_task(\n",
            "\u001b[0;32m/usr/lib/python3.11/asyncio/unix_events.py\u001b[0m in \u001b[0;36mclose\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_finalizing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0msig\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_signal_handlers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/asyncio/selector_events.py\u001b[0m in \u001b[0;36mclose\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_running\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Cannot close a running event loop\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Cannot close a running event loop"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "main()"
      ],
      "metadata": {
        "id": "3X-GOoxBsqz9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}