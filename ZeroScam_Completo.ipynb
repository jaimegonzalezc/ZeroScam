{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "344f9d047a82415eae5d25520621937b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [],
            "layout": "IPY_MODEL_57a488f4f38d4db48ef87ee8b8988954"
          }
        },
        "47ba9942506b4f9f9a067b877decc515": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5100674e9785441bbe40b8663dafa9db",
            "placeholder": "​",
            "style": "IPY_MODEL_bc1c9151c396411e98f7efc414ab88e3",
            "value": "<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"
          }
        },
        "07d96c0bf258491480ab501c27f92dc5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "PasswordModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "PasswordModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "PasswordView",
            "continuous_update": true,
            "description": "Token:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_75ab0302fea54c05946073fdcab67348",
            "placeholder": "​",
            "style": "IPY_MODEL_66f304bcf4734f07a169c5209feee002",
            "value": ""
          }
        },
        "852d14b1d1264880a9846a211a7ef60b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "CheckboxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "CheckboxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "CheckboxView",
            "description": "Add token as git credential?",
            "description_tooltip": null,
            "disabled": false,
            "indent": true,
            "layout": "IPY_MODEL_faa7bf69e91443d8a4dedbd9e1f54b96",
            "style": "IPY_MODEL_8de4edacaa6b436ca2d8ae5c878423b3",
            "value": true
          }
        },
        "02861aa887e54e768d30d86097b73e96": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Login",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_7a674adf0ea040458fd4c081c417a110",
            "style": "IPY_MODEL_97130284ce9a4ea48ac2dd0da4c83810",
            "tooltip": ""
          }
        },
        "da88884cc0274209b132fdbaa72fa22a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bf7bfd94f9c84038a94658998a5a86d3",
            "placeholder": "​",
            "style": "IPY_MODEL_55efe188497d426f9afc6b618191ba8a",
            "value": "\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"
          }
        },
        "57a488f4f38d4db48ef87ee8b8988954": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "center",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "5100674e9785441bbe40b8663dafa9db": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bc1c9151c396411e98f7efc414ab88e3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "75ab0302fea54c05946073fdcab67348": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "66f304bcf4734f07a169c5209feee002": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "faa7bf69e91443d8a4dedbd9e1f54b96": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8de4edacaa6b436ca2d8ae5c878423b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7a674adf0ea040458fd4c081c417a110": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "97130284ce9a4ea48ac2dd0da4c83810": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "bf7bfd94f9c84038a94658998a5a86d3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "55efe188497d426f9afc6b618191ba8a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6836bdc7830e4fc7b0c6e7c6f2281f4a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4922ea9df59443008a73f8cb1f258bfe",
            "placeholder": "​",
            "style": "IPY_MODEL_1870d1bf5715423a991e960a3b2e77c9",
            "value": "Connecting..."
          }
        },
        "4922ea9df59443008a73f8cb1f258bfe": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1870d1bf5715423a991e960a3b2e77c9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e9f1a86ea76e4d73b787fac08f597db7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5060ef17028249f783f8c961f6df5e6a",
              "IPY_MODEL_423f7d49213241bd9daf4fa7f083cb10",
              "IPY_MODEL_9e52930293164fdd904f3205c20702fb"
            ],
            "layout": "IPY_MODEL_ed6ff4a017384d848f4ec9e2b8fb4200"
          }
        },
        "5060ef17028249f783f8c961f6df5e6a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b3c5453e3a9942e69b8e5abb6e09937c",
            "placeholder": "​",
            "style": "IPY_MODEL_7f1d6aa2c1fd4424a2d5bc911ee127aa",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "423f7d49213241bd9daf4fa7f083cb10": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_413ce24651c849aeb1bce9ba6da909b5",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_19c071aeac61459c948ba8ce6689460e",
            "value": 2
          }
        },
        "9e52930293164fdd904f3205c20702fb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cefd59584a764cb08f4f2ecf2795ee2d",
            "placeholder": "​",
            "style": "IPY_MODEL_638a924f8e09441cbde08de9051d637e",
            "value": " 2/2 [00:09&lt;00:00,  4.84s/it]"
          }
        },
        "ed6ff4a017384d848f4ec9e2b8fb4200": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b3c5453e3a9942e69b8e5abb6e09937c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7f1d6aa2c1fd4424a2d5bc911ee127aa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "413ce24651c849aeb1bce9ba6da909b5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "19c071aeac61459c948ba8ce6689460e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "cefd59584a764cb08f4f2ecf2795ee2d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "638a924f8e09441cbde08de9051d637e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#ZeroScam\n",
        "\n"
      ],
      "metadata": {
        "id": "DQrdF4Xi82_f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Instalo librerias"
      ],
      "metadata": {
        "id": "-N0vGEj1BX1k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install chromadb fastapi pyngrok uvicorn\n",
        "!pip install datasets trl\n",
        "!pip uninstall -y bitsandbytes\n",
        "!pip install --upgrade bitsandbytes\n",
        "!pip install --upgrade transformers accelerate\n",
        "!nvidia-smi\n",
        "!pip install accelerate\n",
        "!accelerate config\n",
        "!pip install flash-attn --no-build-isolation\n",
        "!pip install pytesseract\n",
        "!pip install --upgrade python-telegram-bot\n",
        "!pip install dotenv\n",
        "!apt-get update\n",
        "!apt-get install -y tesseract-ocr\n",
        "!pip install pytesseract\n",
        "!tesseract -v"
      ],
      "metadata": {
        "id": "MVMdkCQBHeHj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import re\n",
        "import sys\n",
        "import threading\n",
        "import logging\n",
        "import requests\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from bitsandbytes import BitsAndBytesConfig\n",
        "\n",
        "from fastapi import FastAPI\n",
        "import uvicorn\n",
        "from pyngrok import ngrok\n",
        "\n",
        "import pytesseract\n",
        "from PIL import Image, ImageEnhance\n",
        "import spacy\n",
        "\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()"
      ],
      "metadata": {
        "id": "Erhv3xLHpaa1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CONFIGURACIÓN: Montar Google Drive y Variables\n",
        "drive.mount('/content/drive')\n",
        "BASE_DIR = \"/content/drive/My Drive/Trabajo Final Bootcamp\"\n",
        "NORMATIVA_DIR = os.path.join(BASE_DIR, \"normativa\")\n",
        "EMBEDDINGS_BACKUP_PATH = os.path.join(BASE_DIR, \"embeddings.json\")\n",
        "\n",
        "if not os.path.exists(NORMATIVA_DIR):\n",
        "    raise FileNotFoundError(f\"La carpeta {NORMATIVA_DIR} no existe. Verifica la ruta o crea la carpeta.\")"
      ],
      "metadata": {
        "id": "BN1PvRbIpcAs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Login en Hugging Face\n",
        "notebook_login()"
      ],
      "metadata": {
        "id": "clV8Z482pg--"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CARGA DE MODELOS Y CONFIGURACIÓN DE DISPOSITIVO\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"✅ Usando dispositivo: {device}\")\n",
        "\n",
        "model_name = \"CasiAC/deepseek-r1-8b-ciberseguridad\"\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\"\n",
        ")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "embedding_model = SentenceTransformer('all-MiniLM-L6-v2', device=device)\n",
        "print(\"✅ Modelo cargado 🚀\")"
      ],
      "metadata": {
        "id": "0pSHhiLwpqTF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CONFIGURACIÓN DE CHROMADB Y FASTAPI\n",
        "app = FastAPI()\n",
        "\n",
        "# Configuración de ngrok para exponer el puerto 8000\n",
        "ngrok.set_auth_token(\"2sN2ljFFRN4UpJk7VPL6jPiHVJL_6FafRKrvugJysTGGRV1KB\")\n",
        "public_url = ngrok.connect(8000)\n",
        "print(f\"🔗 ChromaDB API accesible en: {public_url}\")\n",
        "\n",
        "client = chromadb.Client()\n",
        "collection = client.create_collection(\n",
        "    name=\"test\",\n",
        "    metadata={\"hnsw:search_ef\": 100, \"hnsw:construction_ef\": 1000}\n",
        ")\n",
        "\n",
        "@app.get(\"/\")\n",
        "def read_root():\n",
        "    return {\"message\": \"Chroma API está en funcionamiento\"}\n",
        "\n",
        "@app.get(\"/collections\")\n",
        "def get_collections():\n",
        "    return {\"collections\": client.list_collections()}\n",
        "\n",
        "@app.get(\"/collections/{collection_name}\")\n",
        "def get_collection(collection_name: str):\n",
        "    return {\"collection\": client.get_collection(name=collection_name)}\n",
        "\n",
        "@app.post(\"/collections/{collection_name}/add\")\n",
        "def add_to_collection(collection_name: str, item: dict):\n",
        "    coll = client.get_collection(name=collection_name)\n",
        "    coll.add(\n",
        "        documents=[item[\"document\"]],\n",
        "        metadatas=[item.get(\"metadata\", {})],\n",
        "        ids=[item.get(\"id\", \"default_id\")]\n",
        "    )\n",
        "    return {\"message\": f\"Elemento añadido a la colección {collection_name}\"}\n",
        "\n",
        "@app.get(\"/health\")\n",
        "def health_check():\n",
        "    return {\"status\": \"OK\"}\n",
        "\n",
        "def start_server():\n",
        "    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n",
        "\n",
        "server_thread = threading.Thread(target=start_server, daemon=True)\n",
        "server_thread.start()"
      ],
      "metadata": {
        "id": "Vn0w9XXdpqKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# FUNCIONES DE INDEXACIÓN Y GENERACIÓN DE CONTEXTO (RAG)\n",
        "def cargar_documentos_y_embeddings():\n",
        "    embeddings_dict = {}\n",
        "    for archivo in os.listdir(NORMATIVA_DIR):\n",
        "        ruta_json = os.path.join(NORMATIVA_DIR, archivo)\n",
        "        with open(ruta_json, \"r\", encoding=\"utf-8\") as f:\n",
        "            documentos = json.load(f)\n",
        "\n",
        "        for section in documentos.get(\"sections\", []):\n",
        "            doc_id = f\"{archivo}_p{section['page']}\"\n",
        "            content = section[\"content\"]\n",
        "            embedding = embedding_model.encode(content, convert_to_tensor=True).cpu().numpy().tolist()\n",
        "            embeddings_dict[doc_id] = embedding\n",
        "\n",
        "            collection.add(\n",
        "                documents=[content],\n",
        "                metadatas=[{\"title\": documentos.get(\"title\", \"Desconocido\"), \"page\": section[\"page\"]}],\n",
        "                embeddings=[embedding],\n",
        "                ids=[doc_id]\n",
        "            )\n",
        "\n",
        "    with open(EMBEDDINGS_BACKUP_PATH, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(embeddings_dict, f, ensure_ascii=False, indent=4)\n",
        "    print(f\"✅ {len(embeddings_dict)} documentos indexados y guardados en ChromaDB 🚀\")\n",
        "    return embeddings_dict\n",
        "\n",
        "embeddings_dict = cargar_documentos_y_embeddings()\n",
        "\n",
        "def obtener_contexto(pregunta, n_docs=3):\n",
        "    embedding_pregunta = embedding_model.encode([pregunta], convert_to_tensor=True).cpu().numpy().tolist()\n",
        "    resultados = collection.query(query_embeddings=embedding_pregunta, n_results=n_docs)\n",
        "    documents = resultados.get('documents', [])\n",
        "    if not documents:\n",
        "        return \"No se encontraron documentos relevantes.\"\n",
        "\n",
        "    documentos_convertidos = [\n",
        "        \" \".join(map(str, doc)) if isinstance(doc, list) else str(doc)\n",
        "        for doc in documents\n",
        "    ]\n",
        "    return \"\\n\".join(documentos_convertidos)\n",
        "\n",
        "def generar_respuesta_rag(pregunta, max_tokens=300, temperatura=0.1):\n",
        "    contexto = obtener_contexto(pregunta)\n",
        "    entrada = f\"Contexto: {contexto}\\nPregunta: {pregunta}\\nRespuesta:\"\n",
        "    inputs = tokenizer(entrada, return_tensors=\"pt\").to(device)\n",
        "    output = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=max_tokens,\n",
        "        temperature=temperatura,\n",
        "        top_p=0.9,\n",
        "        repetition_penalty=1.05\n",
        "    )\n",
        "    return tokenizer.decode(output[0], skip_special_tokens=True)"
      ],
      "metadata": {
        "id": "leD3qn6epp8V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# FUNCIONES DE OCR Y CONSULTA A VIRUSTOTAL\n",
        "pytesseract.pytesseract.tesseract_cmd = '/usr/bin/tesseract'\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "API_KEY = \"06858db9f480b4aba21a5831457a9b919b1f9014e6f8872ee1f4f7d1a029197c\"\n",
        "HEADERS = {\"x-apikey\": API_KEY}\n",
        "\n",
        "def preprocesar_imagen(imagen):\n",
        "    \"\"\"Convierte la imagen a escala de grises y mejora el contraste.\"\"\"\n",
        "    try:\n",
        "        image = Image.open(imagen).convert(\"L\")\n",
        "        enhancer = ImageEnhance.Contrast(image)\n",
        "        return enhancer.enhance(2.0)\n",
        "    except Exception as e:\n",
        "        return None\n",
        "\n",
        "def extraer_texto_img(imagen):\n",
        "    \"\"\"Extrae y limpia el texto de una imagen.\"\"\"\n",
        "    image = preprocesar_imagen(imagen)\n",
        "    if image is None:\n",
        "        return \"Error al procesar la imagen\"\n",
        "    texto_extraido = pytesseract.image_to_string(image)\n",
        "    return limpiar_texto(texto_extraido)\n",
        "\n",
        "def consultar_ip(ip):\n",
        "    url = f\"https://www.virustotal.com/api/v3/ip_addresses/{ip}\"\n",
        "    response = requests.get(url, headers=HEADERS)\n",
        "    if response.status_code == 200:\n",
        "        data = response.json()\n",
        "        stats = data[\"data\"][\"attributes\"][\"last_analysis_stats\"]\n",
        "        malicious = stats.get(\"malicious\", 0)\n",
        "        if malicious > 0:\n",
        "            veredicto = f\"❌ La IP {ip} ha sido reportada como maliciosa en {malicious} análisis.\"\n",
        "        else:\n",
        "            veredicto = f\"✅ La IP {ip} parece segura.\"\n",
        "        return {\"IP\": ip, \"Veredicto\": veredicto, \"Análisis\": stats}\n",
        "    return {\"error\": f\"Error en la consulta: {response.status_code}\"}\n",
        "\n",
        "def consultar_url(url):\n",
        "    scan_url = \"https://www.virustotal.com/api/v3/urls\"\n",
        "    response = requests.post(scan_url, headers=HEADERS, data={\"url\": url})\n",
        "    if response.status_code == 200:\n",
        "        analysis_id = response.json()[\"data\"][\"id\"]\n",
        "        result_url = f\"https://www.virustotal.com/api/v3/analyses/{analysis_id}\"\n",
        "        result_response = requests.get(result_url, headers=HEADERS)\n",
        "        if result_response.status_code == 200:\n",
        "            data = result_response.json()\n",
        "            stats = data[\"data\"][\"attributes\"][\"stats\"]\n",
        "            malicious = stats.get(\"malicious\", 0)\n",
        "            if malicious > 0:\n",
        "                veredicto = f\"❌ La URL {url} ha sido marcada como maliciosa en {malicious} análisis.\"\n",
        "            else:\n",
        "                veredicto = f\"✅ La URL {url} parece segura.\"\n",
        "            return {\"URL\": url, \"Veredicto\": veredicto, \"Análisis\": stats}\n",
        "    return {\"error\": f\"Error en la consulta: {response.status_code}\"}\n",
        "\n",
        "def limpiar_texto(texto):\n",
        "    texto = texto.strip()\n",
        "    texto = re.sub(r'\\s+', ' ', texto)\n",
        "    return texto.replace(\"\\n\", \" \").replace(\"\\r\", \"\")\n",
        "\n",
        "def analizar_con_modelo(texto_extraido):\n",
        "    \"\"\"Analiza el mensaje para detectar señales de phishing utilizando el modelo.\n",
        "       Responde siempre en español.\n",
        "    \"\"\"\n",
        "    texto_limpio = limpiar_texto(texto_extraido)\n",
        "    system_prompt = \"\"\"\n",
        "      Eres un asistente altamente especializado en ciberseguridad. Tu tarea principal es analizar mensajes y detectar intentos de phishing con precisión.\n",
        "      🔹 REGLAS ESTRICTAS:\n",
        "      1. No inventes información. Basa tu respuesta ÚNICAMENTE en el texto proporcionado.\n",
        "      2. Sé conciso y preciso.\n",
        "      3. La respuesta debe comenzar con \"Phishing\" o \"Not Phishing\", seguido de una breve explicación.\n",
        "      4. Responde siempre en español.\n",
        "    \"\"\"\n",
        "    prompt_modelo = f\"\"\"\n",
        "      Analiza el siguiente mensaje y determina si se trata de un intento de phishing.\n",
        "      MENSAJE A EVALUAR:\n",
        "      {texto_extraido}\n",
        "    \"\"\"\n",
        "    inputs = tokenizer(system_prompt + prompt_modelo, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
        "    outputs = model.generate(\n",
        "        inputs.input_ids,\n",
        "        attention_mask=inputs.attention_mask,\n",
        "        max_new_tokens=300,\n",
        "        temperature=0.15,\n",
        "        do_sample=True,\n",
        "        top_k=10,\n",
        "        top_p=0.7,\n",
        "        repetition_penalty=1.2,\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        "    )\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True).strip()\n",
        "    return response\n",
        "\n",
        "def analizar_prompt(prompt):\n",
        "    \"\"\"\n",
        "    Detecta si el prompt contiene:\n",
        "      - Una imagen (por extensión) para procesar OCR y análisis de phishing.\n",
        "      - Una IP o URL para consulta en VirusTotal.\n",
        "    Devuelve un resultado especial (no None) si se cumple alguno de estos casos.\n",
        "    \"\"\"\n",
        "    if isinstance(prompt, str) and prompt.lower().endswith(('.png', '.jpg', '.jpeg', '.gif', '.bmp', '.webp', '.heic')):\n",
        "        print(f\"🔍 Detectada imagen: {prompt}\")\n",
        "        texto_extraido = extraer_texto_img(prompt)\n",
        "        print(f\"Texto extraído: {texto_extraido}\")\n",
        "        return analizar_con_modelo(texto_extraido)\n",
        "\n",
        "    ip_pattern = r\"\\b(?:\\d{1,3}\\.){3}\\d{1,3}\\b\"\n",
        "    url_pattern = r'(https?://[^\\s]+|www\\.[^\\s]+)'\n",
        "    if re.search(ip_pattern, prompt):\n",
        "        ip = re.search(ip_pattern, prompt).group()\n",
        "        print(f\"🔍 Detectada IP: {ip}\")\n",
        "        return consultar_ip(ip)\n",
        "    if re.search(url_pattern, prompt):\n",
        "        url = re.search(url_pattern, prompt).group()\n",
        "        print(f\"🔍 Detectada URL: {url}\")\n",
        "        return consultar_url(url)\n",
        "    return None"
      ],
      "metadata": {
        "id": "eFZtEvzSp1cX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# FUNCIÓN UNIFICADA DE GENERACIÓN DE RESPUESTA\n",
        "def generate_response(prompt):\n",
        "    \"\"\"\n",
        "    Función unificada que:\n",
        "      1. Verifica si el prompt es especial (imagen, IP, URL) mediante analizar_prompt().\n",
        "      2. Si no es especial, utiliza la generación basada en contexto (RAG).\n",
        "      3. Y si no se encuentra relación en el contexto (por ejemplo, se obtiene \"No se encontraron documentos relevantes\"),\n",
        "         se usa una generación “por defecto”.\n",
        "    \"\"\"\n",
        "    # Paso 1: Verificar si el prompt es especial\n",
        "    resultado_api = analizar_prompt(prompt)\n",
        "    if resultado_api is not None:\n",
        "        return json.dumps(resultado_api, indent=4, ensure_ascii=False)\n",
        "\n",
        "    # Paso 2: Consultar contexto en ChromaDB para RAG\n",
        "    contexto = obtener_contexto(prompt)\n",
        "    if contexto.strip().lower().startswith(\"no se encontraron documentos\"):\n",
        "        # Paso 3: Generación por defecto si no hay contexto relevante\n",
        "        inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
        "        outputs = model.generate(\n",
        "            inputs.input_ids,\n",
        "            attention_mask=inputs.attention_mask,\n",
        "            max_new_tokens=200,\n",
        "            temperature=0.2,\n",
        "            do_sample=True,\n",
        "            top_k=50,\n",
        "            top_p=0.9,\n",
        "            repetition_penalty=1.5,\n",
        "            pad_token_id=tokenizer.eos_token_id,\n",
        "        )\n",
        "        return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    else:\n",
        "        # Si se encontró contexto, usar generación basada en RAG\n",
        "        return generar_respuesta_rag(prompt)\n"
      ],
      "metadata": {
        "id": "j7Wz4ayLp1PF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PRUEBA DE LA FUNCIÓN UNIFICADA\n",
        "# ---------------------------------\n",
        "prompt_usuario = 'image6.png'\n",
        "respuesta_usuario = generate_response(prompt_usuario)\n",
        "print(f\"\\n🔹 Respuesta:\\n{respuesta_usuario}\")"
      ],
      "metadata": {
        "id": "ceOaDqvuqrX3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CONFIGURACIÓN DEL BOT DE TELEGRAM\n",
        "# ---------------------------------\n",
        "load_dotenv()\n",
        "\n",
        "logging.basicConfig(\n",
        "    format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\",\n",
        "    level=logging.INFO,\n",
        "    handlers=[logging.StreamHandler(sys.stdout)]\n",
        ")\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "from telegram import Update\n",
        "from telegram.ext import Application, CommandHandler, MessageHandler, filters, CallbackContext\n",
        "\n",
        "async def start(update: Update, context: CallbackContext) -> None:\n",
        "    await update.message.reply_text(\"¡Hola! Soy tu especialista en ciberseguridad. ¿En qué te puedo ayudar?\")\n",
        "\n",
        "async def handle_message(update: Update, context: CallbackContext) -> None:\n",
        "    user_input = update.message.text\n",
        "    username = update.message.from_user.username\n",
        "    logger.info(f\"Mensaje recibido de {username}: {user_input}\")\n",
        "    response = generate_response(user_input)\n",
        "    await update.message.reply_text(response)\n",
        "\n",
        "def main():\n",
        "    TELEGRAM_TOKEN = \"7047664203:AAEa-JEcZQpv-tDCIdV6ZE_odp4lPTH0Bd8\"\n",
        "    if not TELEGRAM_TOKEN:\n",
        "        logger.error(\"El token de Telegram no está configurado.\")\n",
        "        sys.exit(1)\n",
        "\n",
        "    app_telegram = Application.builder().token(TELEGRAM_TOKEN).build()\n",
        "    app_telegram.add_handler(CommandHandler(\"start\", start))\n",
        "    app_telegram.add_handler(MessageHandler(filters.TEXT & ~filters.COMMAND, handle_message))\n",
        "    logger.info(\"Bot iniciado y ejecutándose...\")\n",
        "    app_telegram.run_polling()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "2ULcOY7Tq3c3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "L46NcVtWrR9O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "l2qe-grSrRjv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4eo1Rf7TrRSB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import re\n",
        "import sys\n",
        "import threading\n",
        "import logging\n",
        "import requests\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from bitsandbytes import BitsAndBytesConfig\n",
        "\n",
        "from fastapi import FastAPI\n",
        "import uvicorn\n",
        "from pyngrok import ngrok\n",
        "\n",
        "import pytesseract\n",
        "from PIL import Image, ImageEnhance\n",
        "import spacy\n",
        "\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# ---------------------------------\n",
        "# CONFIGURACIÓN: Montar Google Drive y Variables\n",
        "# ---------------------------------\n",
        "drive.mount('/content/drive')\n",
        "BASE_DIR = \"/content/drive/My Drive/Trabajo Final Bootcamp\"\n",
        "NORMATIVA_DIR = os.path.join(BASE_DIR, \"normativa\")\n",
        "EMBEDDINGS_BACKUP_PATH = os.path.join(BASE_DIR, \"embeddings.json\")\n",
        "\n",
        "if not os.path.exists(NORMATIVA_DIR):\n",
        "    raise FileNotFoundError(f\"La carpeta {NORMATIVA_DIR} no existe. Verifica la ruta o crea la carpeta.\")\n",
        "\n",
        "notebook_login()  # Login en Hugging Face (ejecutar solo una vez)\n",
        "\n",
        "# ---------------------------------\n",
        "# CARGA DE MODELOS Y CONFIGURACIÓN DE DISPOSITIVO\n",
        "# ---------------------------------\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"✅ Usando dispositivo: {device}\")\n",
        "\n",
        "model_name = \"CasiAC/deepseek-r1-8b-ciberseguridad\"\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\"\n",
        ")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "embedding_model = SentenceTransformer('all-MiniLM-L6-v2', device=device)\n",
        "print(\"✅ Modelo cargado 🚀\")\n",
        "\n",
        "# ---------------------------------\n",
        "# CONFIGURACIÓN DE CHROMADB Y FASTAPI\n",
        "# ---------------------------------\n",
        "app = FastAPI()\n",
        "\n",
        "# Configuración de ngrok para exponer el puerto 8000\n",
        "ngrok.set_auth_token(\"2sN2ljFFRN4UpJk7VPL6jPiHVJL_6FafRKrvugJysTGGRV1KB\")\n",
        "public_url = ngrok.connect(8000)\n",
        "print(f\"🔗 ChromaDB API accesible en: {public_url}\")\n",
        "\n",
        "client = chromadb.Client()\n",
        "collection = client.create_collection(\n",
        "    name=\"test\",\n",
        "    metadata={\"hnsw:search_ef\": 100, \"hnsw:construction_ef\": 1000}\n",
        ")\n",
        "\n",
        "@app.get(\"/\")\n",
        "def read_root():\n",
        "    return {\"message\": \"Chroma API está en funcionamiento\"}\n",
        "\n",
        "@app.get(\"/collections\")\n",
        "def get_collections():\n",
        "    return {\"collections\": client.list_collections()}\n",
        "\n",
        "@app.get(\"/collections/{collection_name}\")\n",
        "def get_collection(collection_name: str):\n",
        "    return {\"collection\": client.get_collection(name=collection_name)}\n",
        "\n",
        "@app.post(\"/collections/{collection_name}/add\")\n",
        "def add_to_collection(collection_name: str, item: dict):\n",
        "    coll = client.get_collection(name=collection_name)\n",
        "    coll.add(\n",
        "        documents=[item[\"document\"]],\n",
        "        metadatas=[item.get(\"metadata\", {})],\n",
        "        ids=[item.get(\"id\", \"default_id\")]\n",
        "    )\n",
        "    return {\"message\": f\"Elemento añadido a la colección {collection_name}\"}\n",
        "\n",
        "@app.get(\"/health\")\n",
        "def health_check():\n",
        "    return {\"status\": \"OK\"}\n",
        "\n",
        "def start_server():\n",
        "    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n",
        "\n",
        "server_thread = threading.Thread(target=start_server, daemon=True)\n",
        "server_thread.start()\n",
        "\n",
        "# ---------------------------------\n",
        "# FUNCIONES DE INDEXACIÓN Y GENERACIÓN DE CONTEXTO (RAG)\n",
        "# ---------------------------------\n",
        "def cargar_documentos_y_embeddings():\n",
        "    embeddings_dict = {}\n",
        "    for archivo in os.listdir(NORMATIVA_DIR):\n",
        "        ruta_json = os.path.join(NORMATIVA_DIR, archivo)\n",
        "        with open(ruta_json, \"r\", encoding=\"utf-8\") as f:\n",
        "            documentos = json.load(f)\n",
        "\n",
        "        for section in documentos.get(\"sections\", []):\n",
        "            doc_id = f\"{archivo}_p{section['page']}\"\n",
        "            content = section[\"content\"]\n",
        "            embedding = embedding_model.encode(content, convert_to_tensor=True).cpu().numpy().tolist()\n",
        "            embeddings_dict[doc_id] = embedding\n",
        "\n",
        "            collection.add(\n",
        "                documents=[content],\n",
        "                metadatas=[{\"title\": documentos.get(\"title\", \"Desconocido\"), \"page\": section[\"page\"]}],\n",
        "                embeddings=[embedding],\n",
        "                ids=[doc_id]\n",
        "            )\n",
        "\n",
        "    with open(EMBEDDINGS_BACKUP_PATH, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(embeddings_dict, f, ensure_ascii=False, indent=4)\n",
        "    print(f\"✅ {len(embeddings_dict)} documentos indexados y guardados en ChromaDB 🚀\")\n",
        "    return embeddings_dict\n",
        "\n",
        "embeddings_dict = cargar_documentos_y_embeddings()\n",
        "\n",
        "def obtener_contexto(pregunta, n_docs=3):\n",
        "    embedding_pregunta = embedding_model.encode([pregunta], convert_to_tensor=True).cpu().numpy().tolist()\n",
        "    resultados = collection.query(query_embeddings=embedding_pregunta, n_results=n_docs)\n",
        "    documents = resultados.get('documents', [])\n",
        "    if not documents:\n",
        "        return \"No se encontraron documentos relevantes.\"\n",
        "\n",
        "    documentos_convertidos = [\n",
        "        \" \".join(map(str, doc)) if isinstance(doc, list) else str(doc)\n",
        "        for doc in documents\n",
        "    ]\n",
        "    return \"\\n\".join(documentos_convertidos)\n",
        "\n",
        "def generar_respuesta_rag(pregunta, max_tokens=300, temperatura=0.1):\n",
        "    contexto = obtener_contexto(pregunta)\n",
        "    entrada = f\"Contexto: {contexto}\\nPregunta: {pregunta}\\nRespuesta:\"\n",
        "    inputs = tokenizer(entrada, return_tensors=\"pt\").to(device)\n",
        "    output = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=max_tokens,\n",
        "        temperature=temperatura,\n",
        "        top_p=0.9,\n",
        "        repetition_penalty=1.05\n",
        "    )\n",
        "    return tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "# ---------------------------------\n",
        "# FUNCIONES DE OCR Y CONSULTA A VIRUSTOTAL\n",
        "# ---------------------------------\n",
        "pytesseract.pytesseract.tesseract_cmd = '/usr/bin/tesseract'\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "API_KEY = \"06858db9f480b4aba21a5831457a9b919b1f9014e6f8872ee1f4f7d1a029197c\"\n",
        "HEADERS = {\"x-apikey\": API_KEY}\n",
        "\n",
        "def preprocesar_imagen(imagen):\n",
        "    \"\"\"Convierte la imagen a escala de grises y mejora el contraste.\"\"\"\n",
        "    try:\n",
        "        image = Image.open(imagen).convert(\"L\")\n",
        "        enhancer = ImageEnhance.Contrast(image)\n",
        "        return enhancer.enhance(2.0)\n",
        "    except Exception as e:\n",
        "        return None\n",
        "\n",
        "def extraer_texto_img(imagen):\n",
        "    \"\"\"Extrae y limpia el texto de una imagen.\"\"\"\n",
        "    image = preprocesar_imagen(imagen)\n",
        "    if image is None:\n",
        "        return \"Error al procesar la imagen\"\n",
        "    texto_extraido = pytesseract.image_to_string(image)\n",
        "    return limpiar_texto(texto_extraido)\n",
        "\n",
        "def consultar_ip(ip):\n",
        "    url = f\"https://www.virustotal.com/api/v3/ip_addresses/{ip}\"\n",
        "    response = requests.get(url, headers=HEADERS)\n",
        "    if response.status_code == 200:\n",
        "        data = response.json()\n",
        "        stats = data[\"data\"][\"attributes\"][\"last_analysis_stats\"]\n",
        "        malicious = stats.get(\"malicious\", 0)\n",
        "        if malicious > 0:\n",
        "            veredicto = f\"❌ La IP {ip} ha sido reportada como maliciosa en {malicious} análisis.\"\n",
        "        else:\n",
        "            veredicto = f\"✅ La IP {ip} parece segura.\"\n",
        "        return {\"IP\": ip, \"Veredicto\": veredicto, \"Análisis\": stats}\n",
        "    return {\"error\": f\"Error en la consulta: {response.status_code}\"}\n",
        "\n",
        "def consultar_url(url):\n",
        "    scan_url = \"https://www.virustotal.com/api/v3/urls\"\n",
        "    response = requests.post(scan_url, headers=HEADERS, data={\"url\": url})\n",
        "    if response.status_code == 200:\n",
        "        analysis_id = response.json()[\"data\"][\"id\"]\n",
        "        result_url = f\"https://www.virustotal.com/api/v3/analyses/{analysis_id}\"\n",
        "        result_response = requests.get(result_url, headers=HEADERS)\n",
        "        if result_response.status_code == 200:\n",
        "            data = result_response.json()\n",
        "            stats = data[\"data\"][\"attributes\"][\"stats\"]\n",
        "            malicious = stats.get(\"malicious\", 0)\n",
        "            if malicious > 0:\n",
        "                veredicto = f\"❌ La URL {url} ha sido marcada como maliciosa en {malicious} análisis.\"\n",
        "            else:\n",
        "                veredicto = f\"✅ La URL {url} parece segura.\"\n",
        "            return {\"URL\": url, \"Veredicto\": veredicto, \"Análisis\": stats}\n",
        "    return {\"error\": f\"Error en la consulta: {response.status_code}\"}\n",
        "\n",
        "def limpiar_texto(texto):\n",
        "    texto = texto.strip()\n",
        "    texto = re.sub(r'\\s+', ' ', texto)\n",
        "    return texto.replace(\"\\n\", \" \").replace(\"\\r\", \"\")\n",
        "\n",
        "def analizar_con_modelo(texto_extraido):\n",
        "    \"\"\"Analiza el mensaje para detectar señales de phishing utilizando el modelo.\n",
        "       Responde siempre en español.\n",
        "    \"\"\"\n",
        "    texto_limpio = limpiar_texto(texto_extraido)\n",
        "    system_prompt = \"\"\"\n",
        "      Eres un asistente altamente especializado en ciberseguridad. Tu tarea principal es analizar mensajes y detectar intentos de phishing con precisión.\n",
        "      🔹 REGLAS ESTRICTAS:\n",
        "      1. No inventes información. Basa tu respuesta ÚNICAMENTE en el texto proporcionado.\n",
        "      2. Sé conciso y preciso.\n",
        "      3. La respuesta debe comenzar con \"Phishing\" o \"Not Phishing\", seguido de una breve explicación.\n",
        "      4. Responde siempre en español.\n",
        "    \"\"\"\n",
        "    prompt_modelo = f\"\"\"\n",
        "      Analiza el siguiente mensaje y determina si se trata de un intento de phishing.\n",
        "      MENSAJE A EVALUAR:\n",
        "      {texto_extraido}\n",
        "    \"\"\"\n",
        "    inputs = tokenizer(system_prompt + prompt_modelo, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
        "    outputs = model.generate(\n",
        "        inputs.input_ids,\n",
        "        attention_mask=inputs.attention_mask,\n",
        "        max_new_tokens=300,\n",
        "        temperature=0.15,\n",
        "        do_sample=True,\n",
        "        top_k=10,\n",
        "        top_p=0.7,\n",
        "        repetition_penalty=1.2,\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        "    )\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True).strip()\n",
        "    return response\n",
        "\n",
        "def analizar_prompt(prompt):\n",
        "    \"\"\"\n",
        "    Detecta si el prompt contiene:\n",
        "      - Una imagen (por extensión) para procesar OCR y análisis de phishing.\n",
        "      - Una IP o URL para consulta en VirusTotal.\n",
        "    Devuelve un resultado especial (no None) si se cumple alguno de estos casos.\n",
        "    \"\"\"\n",
        "    if isinstance(prompt, str) and prompt.lower().endswith(('.png', '.jpg', '.jpeg', '.gif', '.bmp', '.webp', '.heic')):\n",
        "        print(f\"🔍 Detectada imagen: {prompt}\")\n",
        "        texto_extraido = extraer_texto_img(prompt)\n",
        "        print(f\"Texto extraído: {texto_extraido}\")\n",
        "        return analizar_con_modelo(texto_extraido)\n",
        "\n",
        "    ip_pattern = r\"\\b(?:\\d{1,3}\\.){3}\\d{1,3}\\b\"\n",
        "    url_pattern = r'(https?://[^\\s]+|www\\.[^\\s]+)'\n",
        "    if re.search(ip_pattern, prompt):\n",
        "        ip = re.search(ip_pattern, prompt).group()\n",
        "        print(f\"🔍 Detectada IP: {ip}\")\n",
        "        return consultar_ip(ip)\n",
        "    if re.search(url_pattern, prompt):\n",
        "        url = re.search(url_pattern, prompt).group()\n",
        "        print(f\"🔍 Detectada URL: {url}\")\n",
        "        return consultar_url(url)\n",
        "    return None\n",
        "\n",
        "# ---------------------------------\n",
        "# FUNCIÓN UNIFICADA DE GENERACIÓN DE RESPUESTA\n",
        "# ---------------------------------\n",
        "def generate_response(prompt):\n",
        "    \"\"\"\n",
        "    Función unificada que:\n",
        "      1. Verifica si el prompt es especial (imagen, IP, URL) mediante analizar_prompt().\n",
        "      2. Si no es especial, utiliza la generación basada en contexto (RAG).\n",
        "      3. Y si no se encuentra relación en el contexto (por ejemplo, se obtiene \"No se encontraron documentos relevantes\"),\n",
        "         se usa una generación “por defecto”.\n",
        "    \"\"\"\n",
        "    # Paso 1: Verificar si el prompt es especial\n",
        "    resultado_api = analizar_prompt(prompt)\n",
        "    if resultado_api is not None:\n",
        "        return json.dumps(resultado_api, indent=4, ensure_ascii=False)\n",
        "\n",
        "    # Paso 2: Consultar contexto en ChromaDB para RAG\n",
        "    contexto = obtener_contexto(prompt)\n",
        "    if contexto.strip().lower().startswith(\"no se encontraron documentos\"):\n",
        "        # Paso 3: Generación por defecto si no hay contexto relevante\n",
        "        inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
        "        outputs = model.generate(\n",
        "            inputs.input_ids,\n",
        "            attention_mask=inputs.attention_mask,\n",
        "            max_new_tokens=200,\n",
        "            temperature=0.2,\n",
        "            do_sample=True,\n",
        "            top_k=50,\n",
        "            top_p=0.9,\n",
        "            repetition_penalty=1.5,\n",
        "            pad_token_id=tokenizer.eos_token_id,\n",
        "        )\n",
        "        return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    else:\n",
        "        # Si se encontró contexto, usar generación basada en RAG\n",
        "        return generar_respuesta_rag(prompt)\n",
        "\n",
        "# ---------------------------------\n",
        "# PRUEBA DE LA FUNCIÓN UNIFICADA\n",
        "# ---------------------------------\n",
        "prompt_usuario = 'image6.png'\n",
        "respuesta_usuario = generate_response(prompt_usuario)\n",
        "print(f\"\\n🔹 Respuesta:\\n{respuesta_usuario}\")\n",
        "\n",
        "# ---------------------------------\n",
        "# CONFIGURACIÓN DEL BOT DE TELEGRAM\n",
        "# ---------------------------------\n",
        "load_dotenv()\n",
        "\n",
        "logging.basicConfig(\n",
        "    format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\",\n",
        "    level=logging.INFO,\n",
        "    handlers=[logging.StreamHandler(sys.stdout)]\n",
        ")\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "from telegram import Update\n",
        "from telegram.ext import Application, CommandHandler, MessageHandler, filters, CallbackContext\n",
        "\n",
        "async def start(update: Update, context: CallbackContext) -> None:\n",
        "    await update.message.reply_text(\"¡Hola! Soy tu especialista en ciberseguridad. ¿En qué te puedo ayudar?\")\n",
        "\n",
        "async def handle_message(update: Update, context: CallbackContext) -> None:\n",
        "    user_input = update.message.text\n",
        "    username = update.message.from_user.username\n",
        "    logger.info(f\"Mensaje recibido de {username}: {user_input}\")\n",
        "    response = generate_response(user_input)\n",
        "    await update.message.reply_text(response)\n",
        "\n",
        "def main():\n",
        "    TELEGRAM_TOKEN = \"7047664203:AAEa-JEcZQpv-tDCIdV6ZE_odp4lPTH0Bd8\"\n",
        "    if not TELEGRAM_TOKEN:\n",
        "        logger.error(\"El token de Telegram no está configurado.\")\n",
        "        sys.exit(1)\n",
        "\n",
        "    app_telegram = Application.builder().token(TELEGRAM_TOKEN).build()\n",
        "    app_telegram.add_handler(CommandHandler(\"start\", start))\n",
        "    app_telegram.add_handler(MessageHandler(filters.TEXT & ~filters.COMMAND, handle_message))\n",
        "    logger.info(\"Bot iniciado y ejecutándose...\")\n",
        "    app_telegram.run_polling()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lt2W1RFiodOV",
        "outputId": "cffc2e82-25df-481b-e971-31b04fff2083"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rHit:1 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "\r0% [Connecting to security.ubuntu.com] [Waiting for headers] [Connected to r2u.stat.illinois.edu (19\r                                                                                                    \rGet:2 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n",
            "\r0% [Waiting for headers] [Connecting to security.ubuntu.com] [2 InRelease 3,632 B/3,632 B 100%] [Wai\r0% [Waiting for headers] [Connecting to security.ubuntu.com] [Waiting for headers] [Waiting for head\r                                                                                                    \rGet:3 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "\r0% [3 InRelease 5,484 B/128 kB 4%] [Connecting to security.ubuntu.com] [Waiting for headers] [Waitin\r                                                                                                    \rGet:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "Get:5 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Get:7 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Get:8 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ Packages [68.9 kB]\n",
            "Get:9 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [1,338 kB]\n",
            "Get:10 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,665 kB]\n",
            "Get:11 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,533 kB]\n",
            "Get:12 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease [18.1 kB]\n",
            "Get:13 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [8,708 kB]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [2,956 kB]\n",
            "Get:15 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease [24.3 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [3,830 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu jammy-backports/universe amd64 Packages [35.2 kB]\n",
            "Hit:18 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:19 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy/main amd64 Packages [34.0 kB]\n",
            "Get:20 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,235 kB]\n",
            "Get:21 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy/main amd64 Packages [57.8 kB]\n",
            "Get:22 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [2,652 kB]\n",
            "Get:23 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [3,694 kB]\n",
            "Fetched 29.2 MB in 2s (13.0 MB/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  tesseract-ocr-eng tesseract-ocr-osd\n",
            "The following NEW packages will be installed:\n",
            "  tesseract-ocr tesseract-ocr-eng tesseract-ocr-osd\n",
            "0 upgraded, 3 newly installed, 0 to remove and 119 not upgraded.\n",
            "Need to get 4,816 kB of archives.\n",
            "After this operation, 15.6 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-eng all 1:4.00~git30-7274cfa-1.1 [1,591 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-osd all 1:4.00~git30-7274cfa-1.1 [2,990 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr amd64 4.1.1-2.1build1 [236 kB]\n",
            "Fetched 4,816 kB in 1s (9,481 kB/s)\n",
            "Selecting previously unselected package tesseract-ocr-eng.\n",
            "(Reading database ... 124788 files and directories currently installed.)\n",
            "Preparing to unpack .../tesseract-ocr-eng_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-eng (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-osd.\n",
            "Preparing to unpack .../tesseract-ocr-osd_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-osd (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr.\n",
            "Preparing to unpack .../tesseract-ocr_4.1.1-2.1build1_amd64.deb ...\n",
            "Unpacking tesseract-ocr (4.1.1-2.1build1) ...\n",
            "Setting up tesseract-ocr-eng (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-osd (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr (4.1.1-2.1build1) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Requirement already satisfied: pytesseract in /usr/local/lib/python3.11/dist-packages (0.3.13)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.11/dist-packages (from pytesseract) (24.2)\n",
            "Requirement already satisfied: Pillow>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from pytesseract) (11.1.0)\n",
            "tesseract 4.1.1\n",
            " leptonica-1.82.0\n",
            "  libgif 5.1.9 : libjpeg 8d (libjpeg-turbo 2.1.1) : libpng 1.6.37 : libtiff 4.3.0 : zlib 1.2.11 : libwebp 1.2.2 : libopenjp2 2.4.0\n",
            " Found AVX512BW\n",
            " Found AVX512F\n",
            " Found AVX2\n",
            " Found AVX\n",
            " Found FMA\n",
            " Found SSE\n",
            " Found libarchive 3.6.0 zlib/1.2.11 liblzma/5.2.5 bz2lib/1.0.8 liblz4/1.9.3 libzstd/1.4.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import torch\n",
        "import requests\n",
        "import chromadb\n",
        "from pyngrok import ngrok\n",
        "from fastapi import FastAPI\n",
        "from google.colab import drive\n",
        "from huggingface_hub import notebook_login\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import uvicorn\n",
        "import threading\n",
        "from datasets import load_dataset, Dataset, DatasetDict\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    TrainingArguments,\n",
        "    BitsAndBytesConfig\n",
        ")\n",
        "from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model\n",
        "from trl import SFTTrainer\n",
        "import random\n",
        "import pytesseract\n",
        "import spacy\n",
        "import re\n",
        "from PIL import Image\n",
        "from google.colab import files\n",
        "import sys\n",
        "import logging\n",
        "from telegram import Update\n",
        "from telegram.ext import Application, CommandHandler, MessageHandler, filters, CallbackContext\n",
        "from dotenv import load_dotenv\n",
        "from PIL import Image, ImageEnhance\n",
        "import re\n",
        "import requests"
      ],
      "metadata": {
        "id": "mvcf9YDH8oqp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 🚀 Montar Google Drive para cargar JSON\n",
        "drive.mount('/content/drive')\n",
        "BASE_DIR = \"/content/drive/My Drive/Trabajo Final Bootcamp\"\n",
        "NORMATIVA_DIR = os.path.join(BASE_DIR, \"normativa\")\n",
        "EMBEDDINGS_BACKUP_PATH = os.path.join(BASE_DIR, \"embeddings.json\")"
      ],
      "metadata": {
        "id": "EX7hb_HuJZax",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "264e23a3-4755-4ad7-ec79-04a2fcf89078"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Verificar que la carpeta de normativa exista\n",
        "if not os.path.exists(NORMATIVA_DIR):\n",
        "    raise FileNotFoundError(f\"La carpeta {NORMATIVA_DIR} no existe. Verifica la ruta o crea la carpeta.\")"
      ],
      "metadata": {
        "id": "5rF5j2t53jvR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Login en Hugging Face (ejecutar solo una vez)\n",
        "notebook_login()"
      ],
      "metadata": {
        "id": "r3KImBLND22c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17,
          "referenced_widgets": [
            "344f9d047a82415eae5d25520621937b",
            "47ba9942506b4f9f9a067b877decc515",
            "07d96c0bf258491480ab501c27f92dc5",
            "852d14b1d1264880a9846a211a7ef60b",
            "02861aa887e54e768d30d86097b73e96",
            "da88884cc0274209b132fdbaa72fa22a",
            "57a488f4f38d4db48ef87ee8b8988954",
            "5100674e9785441bbe40b8663dafa9db",
            "bc1c9151c396411e98f7efc414ab88e3",
            "75ab0302fea54c05946073fdcab67348",
            "66f304bcf4734f07a169c5209feee002",
            "faa7bf69e91443d8a4dedbd9e1f54b96",
            "8de4edacaa6b436ca2d8ae5c878423b3",
            "7a674adf0ea040458fd4c081c417a110",
            "97130284ce9a4ea48ac2dd0da4c83810",
            "bf7bfd94f9c84038a94658998a5a86d3",
            "55efe188497d426f9afc6b618191ba8a",
            "6836bdc7830e4fc7b0c6e7c6f2281f4a",
            "4922ea9df59443008a73f8cb1f258bfe",
            "1870d1bf5715423a991e960a3b2e77c9"
          ]
        },
        "outputId": "a46982c7-2383-471a-fd19-89f97c615ea7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "344f9d047a82415eae5d25520621937b"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cargo los modelos"
      ],
      "metadata": {
        "id": "r3n-WUsABkxT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 🚀 Configurar GPU A100\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"✅ Usando dispositivo: {device}\")\n",
        "\n",
        "# 🚀 Cargar Modelo y Tokenizador\n",
        "model_name = \"CasiAC/deepseek-r1-8b-ciberseguridad\"\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\"\n",
        ")\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name, quantization_config=bnb_config, device_map=\"auto\", trust_remote_code=True)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "embedding_model = SentenceTransformer('all-MiniLM-L6-v2', device=device)\n",
        "print(\"✅ Modelo cargado 🚀\")"
      ],
      "metadata": {
        "id": "DahgMeIc8nBx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 188,
          "referenced_widgets": [
            "e9f1a86ea76e4d73b787fac08f597db7",
            "5060ef17028249f783f8c961f6df5e6a",
            "423f7d49213241bd9daf4fa7f083cb10",
            "9e52930293164fdd904f3205c20702fb",
            "ed6ff4a017384d848f4ec9e2b8fb4200",
            "b3c5453e3a9942e69b8e5abb6e09937c",
            "7f1d6aa2c1fd4424a2d5bc911ee127aa",
            "413ce24651c849aeb1bce9ba6da909b5",
            "19c071aeac61459c948ba8ce6689460e",
            "cefd59584a764cb08f4f2ecf2795ee2d",
            "638a924f8e09441cbde08de9051d637e"
          ]
        },
        "outputId": "21a022f9-0ab3-4634-a67e-06e290ac827a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Usando dispositivo: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e9f1a86ea76e4d73b787fac08f597db7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Modelo cargado 🚀\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Configurar ChromaDB en un contenedor Docker"
      ],
      "metadata": {
        "id": "Nlc6y1rZdcvS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Iniciar el servidor FastAPI\n",
        "app = FastAPI()"
      ],
      "metadata": {
        "id": "Y1XzY-eoOKKh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Configuración de ngrok para exponer el puerto 8000\n",
        "ngrok.set_auth_token(\"2sN2ljFFRN4UpJk7VPL6jPiHVJL_6FafRKrvugJysTGGRV1KB\")\n",
        "public_url = ngrok.connect(8000)\n",
        "print(f\"🔗 ChromaDB API accesible en: {public_url}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VyG0nAwrBY8R",
        "outputId": "e686c6db-ae6a-4e10-8748-fa61c2615628"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔗 ChromaDB API accesible en: NgrokTunnel: \"https://74dc-35-188-84-114.ngrok-free.app\" -> \"http://localhost:8000\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Crear cliente y colección en ChromaDB ---\n",
        "client = chromadb.Client()"
      ],
      "metadata": {
        "id": "hJDB44B5AicR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Se unifica la variable como \"collection\" para mayor consistencia\n",
        "collection = client.create_collection(name=\"test\", metadata={\"hnsw:search_ef\": 100, \"hnsw:construction_ef\": 1000})"
      ],
      "metadata": {
        "id": "dg_ahHV1Og8t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Definición de endpoints de FastAPI ---\n",
        "@app.get(\"/\")\n",
        "def read_root():\n",
        "    return {\"message\": \"Chroma API is running!\"}\n",
        "\n",
        "@app.get(\"/collections\")\n",
        "def get_collections():\n",
        "    collections = client.list_collections()\n",
        "    return {\"collections\": collections}\n",
        "\n",
        "@app.get(\"/collections/{collection_name}\")\n",
        "def get_collection(collection_name: str):\n",
        "    coll = client.get_collection(name=collection_name)\n",
        "    return {\"collection\": coll}\n",
        "\n",
        "@app.post(\"/collections/{collection_name}/add\")\n",
        "def add_to_collection(collection_name: str, item: dict):\n",
        "    coll = client.get_collection(name=collection_name)\n",
        "    coll.add(\n",
        "        documents=[item[\"document\"]],\n",
        "        metadatas=[item.get(\"metadata\", {})],\n",
        "        ids=[item.get(\"id\", \"default_id\")]\n",
        "    )\n",
        "    return {\"message\": f\"Item added to collection {collection_name}\"}\n",
        "\n",
        "@app.get(\"/health\")\n",
        "def health_check():\n",
        "    return {\"status\": \"OK\"}"
      ],
      "metadata": {
        "id": "eLzIxdWfEAOx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Iniciar el servidor FastAPI en un hilo de fondo para no bloquear Colab\n",
        "def start_server():\n",
        "    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n",
        "\n",
        "server_thread = threading.Thread(target=start_server, daemon=True)\n",
        "server_thread.start()"
      ],
      "metadata": {
        "id": "lrxWY6dUPEJm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cargar documentos JSON y Generar Embeddings"
      ],
      "metadata": {
        "id": "c4B6cf3-Eh_H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 🚀 Cargar Documentos y Guardar Embeddings en ChromaDB\n",
        "def cargar_documentos_y_embeddings():\n",
        "    embeddings_dict = {}\n",
        "    for archivo in os.listdir(NORMATIVA_DIR):\n",
        "        ruta_json = os.path.join(NORMATIVA_DIR, archivo)\n",
        "        with open(ruta_json, \"r\", encoding=\"utf-8\") as f:\n",
        "            documentos = json.load(f)\n",
        "\n",
        "        for section in documentos.get(\"sections\", []):\n",
        "            doc_id = f\"{archivo}_p{section['page']}\"\n",
        "            content = section[\"content\"]\n",
        "            embedding = embedding_model.encode(content, convert_to_tensor=True).cpu().numpy().tolist()\n",
        "            embeddings_dict[doc_id] = embedding\n",
        "\n",
        "            # Enviar embedding a ChromaDB\n",
        "            collection.add(\n",
        "                documents=[content],\n",
        "                metadatas=[{\"title\": documentos.get(\"title\", \"Desconocido\"), \"page\": section[\"page\"]}],\n",
        "                embeddings=[embedding],\n",
        "                ids=[doc_id]\n",
        "            )\n",
        "\n",
        "    # Guardar copia en Drive\n",
        "    with open(EMBEDDINGS_BACKUP_PATH, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(embeddings_dict, f, ensure_ascii=False, indent=4)\n",
        "    print(f\"✅ {len(embeddings_dict)} documentos indexados y guardados en ChromaDB 🚀\")\n",
        "    return embeddings_dict\n",
        "\n",
        "embeddings_dict = cargar_documentos_y_embeddings()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7BlxM3XWV44A",
        "outputId": "72d7a5d7-d7ee-4573-b7fe-48405a8b24a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ 215 documentos indexados y guardados en ChromaDB 🚀\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implementar el RAG"
      ],
      "metadata": {
        "id": "jvqaJOPtgXsJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 🚀 Función para Obtener Contexto desde ChromaDB\n",
        "def obtener_contexto(pregunta, n_docs=3):\n",
        "    embedding_pregunta = embedding_model.encode([pregunta], convert_to_tensor=True).cpu().numpy().tolist()\n",
        "    resultados = collection.query(\n",
        "        query_embeddings=embedding_pregunta,\n",
        "        n_results=n_docs\n",
        "    )\n",
        "    documents = resultados.get('documents', [])\n",
        "    if not documents:\n",
        "        return \"No se encontraron documentos relevantes.\"\n",
        "\n",
        "    # Convertir cada documento a cadena: si es una lista, unir sus elementos; si no, convertir a string\n",
        "    documentos_convertidos = []\n",
        "    for doc in documents:\n",
        "        if isinstance(doc, list):\n",
        "            documentos_convertidos.append(\" \".join(map(str, doc)))\n",
        "        else:\n",
        "            documentos_convertidos.append(str(doc))\n",
        "\n",
        "    return \"\\n\".join(documentos_convertidos)\n"
      ],
      "metadata": {
        "id": "WWGkxSa1f-_p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 🚀 Función para Generar Respuesta\n",
        "def generar_respuesta(pregunta, max_tokens=300, temperatura=0.1):\n",
        "    contexto = obtener_contexto(pregunta)\n",
        "    entrada = f\"Contexto: {contexto}\\nPregunta: {pregunta}\\nRespuesta:\"\n",
        "    inputs = tokenizer(entrada, return_tensors=\"pt\").to(device)\n",
        "    output = model.generate(**inputs, max_new_tokens=max_tokens, temperature=temperatura, top_p=0.9, repetition_penalty=1.05)\n",
        "    respuesta = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "    return respuesta\n"
      ],
      "metadata": {
        "id": "t4MuhUWCL5S3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 🚀 Probar el modelo con el RAG\n",
        "pregunta = \"¿Cómo mejorar la seguridad en mi empresa?\"\n",
        "respuesta = generar_respuesta(pregunta)\n",
        "print(\"🛡️ Respuesta Generada:\")\n",
        "print(respuesta)"
      ],
      "metadata": {
        "id": "nGoZKp9ef-xj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "935fffd6-986c-453f-9fcb-5867882fc019"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🛡️ Respuesta Generada:\n",
            "Contexto: ISO 27001:2022 IMPLEMENTATION GUIDE\n",
            "14\n",
            "SECCIÓN 4: \n",
            "CONTEXTO DE LA \n",
            "ORGANIZACIÓN\n",
            "Contexto interno\n",
            "Los siguientes son ejemplos de las áreas que pueden tenerse \n",
            "en cuenta al evaluar las cuestiones internas que pueden influir \n",
            "en los riesgos del SGSI:\n",
            "• \u0007 Madurez: ¿Es una empresa ágil con un lienzo en blanco en \n",
            "el que trabajar, o una institución con procesos y controles \n",
            "de seguridad establecidos?\n",
            "•\u0007  Cultura organizativa: ¿Es su organización relajada en \n",
            "cuanto a cómo, cuándo y dónde trabaja la gente, o \n",
            "extremadamente reglamentada? \n",
            "• \u0007 Gestión: ¿Existen canales y procesos de comunicación \n",
            "claros entre los principales responsables de la toma de \n",
            "decisiones y el resto de la organización?\n",
            "•\u0007  Tamaño de los recursos: ¿Trabaja con un equipo de \n",
            "seguridad de la información o lo hace todo una persona?\n",
            "•\u0007  Madurez de los recursos: ¿Los recursos disponibles están \n",
            "informados, plenamente formados, son fiables y constantes, \n",
            "o el personal carece de experiencia y cambia \n",
            "constantemente?\n",
            "•\u0007  Formatos de los activos de información: ¿Sus activos de \n",
            "información se almacenan principalmente en formato \n",
            "impreso o electrónicamente en un servidor o en sistemas \n",
            "remotos basados en la nube?\n",
            "•\u0007  Sensibilidad/valor de los activos de información: ¿Su \n",
            "organización tiene que gestionar activos de información \n",
            "muy valiosos?\n",
            "• \u0007 Coherencia: ¿Dispone de procesos uniformes en toda la \n",
            "organización o de una multitud de prácticas operativas \n",
            "diferentes con poca coherencia?\n",
            "• \u0007 Sistemas: ¿Tiene su organización muchos sistemas que \n",
            "funcionan con versiones de software que ya no son \n",
            "compatibles con el fabricante, o mantiene la tecnología más \n",
            "actualizada y mejor disponible?\u0007\n",
            "•\u0007  Complejidad del sistema: ¿Utiliza un sistema principal que \n",
            "hace todo el trabajo pesado, o varios sistemas \n",
            "departamentales con una transferencia de información \n",
            "limitada entre ellos?\n",
            "•\u0007  Espacio físico: ¿Disponen de una oficina propia y segura, o \n",
            "trabajan en un espacio compartido con otras \n",
            "organizaciones, o son una organización exclusivamente \n",
            "remota?\n",
            "Contexto externo\n",
            "Los siguientes son ejemplos de las áreas que pueden \n",
            "tenerse en cuenta al evaluar las cuestiones externas que \n",
            "pueden influir en los riesgos del SGSI:\n",
            "• \u0007 La competencia: ¿Opera en un mercado innovador y en \n",
            "evolución, que requiere actualizaciones de los sistemas \n",
            "para seguir siendo competitivo, o en un mercado maduro y \n",
            "estable con pocas innovaciones?\n",
            "• \u0007 Propietario: ¿Necesita aprobación para mejorar la \n",
            "seguridad física?\n",
            "•\u0007  Reguladores: ¿Existe en su sector la obligación de realizar \n",
            "cambios reglamentarios con regularidad, o hay poca \n",
            "supervisión por parte de los organismos reguladores?\n",
            "• \u0007 Económico/político: ¿Influyen las fluctuaciones monetarias \n",
            "en su organización? ¿Cómo afectan las situaciones \n",
            "geopolíticas a su organización?\n",
            "•\u0007  Consideraciones ambientales: ¿Están sus instalaciones en \n",
            "una llanura inundable y los servidores en un sótano? \n",
            "¿Existen factores que hagan de sus instalaciones un \n",
            "posible objetivo de robo o atentado terrorista (por ejemplo, \n",
            "en un lugar céntrico o cerca de un posible objetivo)?\n",
            "•\u0007  Prevalencia de los ataques a la seguridad de la \n",
            "información: ¿Su organización opera en un sector que sufre \n",
            "ciberataques?\n",
            "• \u0007 Accionistas: ¿Están muy preocupados por la vulnerabilidad \n",
            "de la organización a las violaciones de datos? ¿Hasta qué \n",
            "punto les preocupa el coste de los esfuerzos de la \n",
            "organización por mejorar su seguridad de la información?\n",
            "El propósito de su SGSI es proteger los Activos de Información de su organización, para \n",
            "que pueda alcanzar sus objetivos.\n",
            "La forma de hacerlo y las áreas específicas de prioridad \n",
            "dependerán del contexto en el que opere su organización.  \n",
            "organización:\n",
            "• \u0007 Internamente: las cosas sobre las que la \n",
            "organización tiene cierto control.\n",
            "• \u0007 Externamente: las cosas que la organización no \n",
            "controla directamente.\n",
            "Un análisis cuidadoso del entorno en el que opera su \n",
            "organización es fundamental para identificar los \n",
            "riesgos inherentes a la seguridad de sus activos de \n",
            "información. El análisis es la base que le permitirá \n",
            "evaluar qué procesos debe considerar añadir o \n",
            "reforzar para construir un SGSI eficaz. ISO 27001:2022 IMPLEMENTATION GUIDE\n",
            "16\n",
            "SECCIÓN 5:  \n",
            "LIDERAZGO\n",
            "Política de seguridad info.\n",
            "Una responsabilidad vital de la dirección es establecer y \n",
            "documentar una Política de Seguridad de la Información (PSI) \n",
            "que esté alineada con los objetivos clave de la organización. \n",
            "Debe incluir objetivos o un marco para establecerlos. Para \n",
            "demostrar que está alineada con el contexto de la \n",
            "organización y los requisitos de las principales partes \n",
            "interesadas, se recomienda que haga referencia o contenga \n",
            "un resumen de los principales problemas y requisitos que \n",
            "debe gestionar. También debe incluir el compromiso de:\n",
            "•\u0007  Cumplir los requisitos aplicables en materia de seguridad de \n",
            "la información, como los requisitos legales, las expectativas \n",
            "de los clientes y los compromisos contractuales.\n",
            "•\u0007  La mejora continua de su SGSI.\n",
            "El PSI puede hacer referencia a, o incluir sub-políticas que \n",
            "cubran, los controles clave del SGSI de la organización. \n",
            "Algunos ejemplos son: la selección de proveedores críticos \n",
            "para la seguridad de la información, la contratación y \n",
            "formación de los empleados, clear desk y clear screen, \n",
            "controles criptográficos, controles de acceso, etc.\n",
            "Para demostrar la importancia del PSI, es aconsejable que lo \n",
            "autorice el miembro de mayor rango de su Alta Dirección o \n",
            "cada uno de los miembros del equipo de Alta Dirección.\n",
            "CONSEJO: Para asegurarse de que su PSI está bien \n",
            "comunicado y a disposición de las partes interesadas, \n",
            "recomendamos:\n",
            "•\u0007  IInclúyala en los paquetes de iniciación y en las \n",
            "presentaciones para nuevos empleados y contratistas.\n",
            "•\u0007  Publique la declaración clave en los tablones de anuncios \n",
            "internos, las intranets y el sitio web de su organización.\n",
            "•\u0007  Haga que su cumplimiento y/o apoyo sea un requisito \n",
            "contractual para empleados, contratistas y proveedores \n",
            "críticos para la seguridad de la información.\n",
            "Funciones y responsabilidades\n",
            "Para que las actividades de seguridad de la información \n",
            "formen parte de las actividades de la mayoría de las personas \n",
            "de la organización, las responsabilidades y las obligaciones \n",
            "de rendir cuentas deben definirse y comunicarse claramente.\n",
            "Aunque la norma no exige la designación de un \n",
            "representante de seguridad de la información, puede ser útil \n",
            "para algunas organizaciones nombrar a uno que dirija un \n",
            "equipo de seguridad de la información para coordinar la \n",
            "formación, supervisar los controles e informar sobre el \n",
            "funcionamiento del SGSI a la alta dirección.  Es posible que \n",
            "esta persona ya sea responsable de la protección de datos.\n",
            "Sin embargo, para desempeñar su función con eficacia, lo \n",
            "ideal es que forme parte del equipo de alta dirección y que \n",
            "tenga sólidos conocimientos técnicos sobre gestión de la \n",
            "seguridad de la información.\n",
            "Evidenciar liderazgo al auditor \n",
            "La Dirección serán aquellos que establecen la dirección \n",
            "estratégica y aprueban la asignación de recursos para la \n",
            "organización o área de negocio con el alcance de su SGSI. \n",
            "Dependiendo de cómo esté estructurada su organización, \n",
            "estas personas pueden ser el equipo directivo diario. Un \n",
            "auditor normalmente pondrá a prueba el liderazgo mediante \n",
            "una entrevista, y evaluará su nivel de implicación en el:\n",
            "•\u0007  Evaloración de riesgos y oportunidades.\n",
            "•\u0007  Establecimiento y comunicación de políticas.\n",
            "•\u0007  Fijación y comunicación de objetivos.\n",
            "•\u0007  Revisión y comunicación del rendimiento del sistema.\n",
            "•\u0007  Asignación de recursos, responsabilidades y obligaciones \n",
            "adecuadas.\n",
            "CONSEJO: Antes de su auditoría externa, identifique quién \n",
            "de la alta dirección se reunirá con el auditor externo. \n",
            "Prepárelos con un simulacro de entrevista que incluya las \n",
            "preguntas que espera que les hagan.\n",
            "El liderazgo en este contexto significa la \n",
            "participación en el establecimiento de la \n",
            "dirección del SGSI, su aplicación y \n",
            "provisión de recursos. Esto incluye:\n",
            "•\u0007  Garantizar que los objetivos del \n",
            "SGSI sean claros y estén alineados \n",
            "con la estrategia general.\n",
            "•\u0007  Claridad en las responsabilidades y \n",
            "la rendición de cuentas.\n",
            "•\u0007  El pensamiento basado en el riesgo \n",
            "está en el centro de toda toma de \n",
            "decisiones\n",
            "•\u0007  Comunicación clara de esta \n",
            "información a todas las personas \n",
            "dentro del ámbito de su SGSI.\n",
            "La norma ISO 27001 concede gran \n",
            "importancia al compromiso activo de \n",
            "la Dirección en el SGSI, partiendo de \n",
            "la base de que el compromiso de la \n",
            "Dirección es crucial para garantizar la \n",
            "implantación efectiva y el \n",
            "mantenimiento de un SGSI eficaz por \n",
            "parte de los empleados.\n",
            "La importancia del liderazgo d) cuando el encargado del tratamiento no sea una institución u organismo de la Unión, normas corporativas vinculantes, \n",
            "códigos de conducta o mecanismos de certificación con arreglo al artículo 46, apartado 2, letras b), e) y f), del \n",
            "Reglamento (UE) 2016/679.\n",
            "3.\n",
            "Siempre que exista autorización del Supervisor Europeo de Protección de Datos, las garantías adecuadas referidas en el \n",
            "apartado 1 también podrán ser aportadas, en particular, mediante:\n",
            "a) cláusulas contractuales entre el responsable o el encargado y el responsable, encargado o destinatario de los datos \n",
            "personales en el tercer país u organización internacional, o\n",
            "b) disposiciones que se incorporen en acuerdos administrativos entre las autoridades u organismos públicos que incluyan \n",
            "derechos efectivos y exigibles para los interesados.\n",
            "4.\n",
            "Las autorizaciones concedidas por el Supervisor Europeo de Protección de Datos de conformidad con el artículo 9, \n",
            "apartado 7, del Reglamento (CE) n.o 45/2001 seguirán siendo válidas hasta que hayan sido modificadas, sustituidas o \n",
            "derogadas, en caso necesario, por este.\n",
            "5.\n",
            "Las instituciones y organismos de la Unión informarán al Supervisor Europeo de Protección de Datos de las categorías \n",
            "de casos en que el presente artículo haya sido aplicado.\n",
            "Artículo 49\n",
            "Transferencias o comunicaciones no autorizadas por el Derecho de la Unión\n",
            "Cualquier sentencia de un órgano jurisdiccional o decisión de una autoridad administrativa de un tercer país que exijan que \n",
            "un responsable o encargado del tratamiento transfiera o comunique datos personales únicamente será reconocida o \n",
            "ejecutable en cualquier modo si se basa en un acuerdo internacional, como un tratado de asistencia jurídica mutua, vigente \n",
            "entre el tercer país requirente y la Unión, sin perjuicio de otros motivos para la transferencia al amparo del presente \n",
            "capítulo.\n",
            "Artículo 50\n",
            "Excepciones para situaciones específicas\n",
            "1.\n",
            "A falta de una decisión de adecuación de conformidad con el artículo 45, apartado 3, del Reglamento (UE) 2016/679, \n",
            "o el artículo 36, apartado 3, de la Directiva (UE) 2016/680, o de garantías adecuadas de conformidad con el artículo 48 del \n",
            "presente Reglamento, solo podrá realizarse una transferencia o una serie de transferencias de datos personales a un tercer \n",
            "país o una organización internacional si se cumple alguna de las condiciones siguientes:\n",
            "a) el interesado haya prestado explícitamente su consentimiento a la transferencia propuesta, tras haber sido informado de \n",
            "los posibles riesgos para él de dichas transferencias debido a la ausencia de una decisión de adecuación y de garantías \n",
            "adecuadas;\n",
            "b) la transferencia sea necesaria para el cumplimiento de un contrato entre el interesado y el responsable del tratamiento o \n",
            "para la ejecución de medidas precontractuales adoptadas a solicitud del interesado;\n",
            "c) la transferencia sea necesaria para la celebración o el cumplimiento de un contrato, en interés del interesado, entre el \n",
            "responsable del tratamiento y otra persona física o jurídica;\n",
            "d) la transferencia sea necesaria por razones importantes de interés público;\n",
            "e) la transferencia sea necesaria para la formulación, el ejercicio o la defensa de reclamaciones; o\n",
            "f) la transferencia sea necesaria para proteger los intereses vitales del interesado o de otras personas, cuando el interesado \n",
            "esté física o jurídicamente incapacitado para prestar su consentimiento; o\n",
            "g) la transferencia se realice desde un registro que, con arreglo al Derecho de la Unión, tenga por objeto proporcionar \n",
            "información al público y que esté disponible para consulta del público en general o de cualquier persona que pueda \n",
            "demostrar un interés legítimo, pero solo en la medida en que en ese caso particular se cumplan las condiciones que \n",
            "establece el Derecho de la Unión para la consulta.\n",
            "2.\n",
            "Las letras a), b) y c) del apartado 1 no serán aplicables a las actividades llevadas a cabo por las instituciones y \n",
            "organismos de la Unión en el ejercicio de sus potestades públicas.\n",
            "3.\n",
            "El interés público indicado en el apartado 1, letra d), será reconocido por el Derecho de la Unión.\n",
            "4.\n",
            "Una transferencia efectuada de conformidad con el apartado 1, letra g), no abarcará la totalidad de los datos \n",
            "personales ni categorías enteras de datos personales contenidos en el registro, a menos que así lo autorice el Derecho de la \n",
            "Unión. Si la finalidad del registro es la consulta por parte de personas que tengan un interés legítimo, la transferencia solo se \n",
            "efectuará a solicitud de dichas personas o si estas han de ser las destinatarias.\n",
            "21.11.2018\n",
            "ES\n",
            "Diario Oficial de la Unión Europea\n",
            "L 295/79\n",
            "Pregunta: ¿Cómo mejorar la seguridad en mi empresa?\n",
            "Respuesta: Aprovecha de la guía de implementación de ISO 27001:2022 para identificar los riesgos y fortalecer tu SGSI.\n",
            "\n",
            "Contexto interno\n",
            "Los siguientes son ejemplos de las áreas que pueden tenerse en cuenta al evaluar las cuestiones internas que pueden influir en los riesgos del SGSI:\n",
            "•  Madurez: ¿Es una empresa ágil con un lienzo en blanco en el que trabajar, o una institución con procesos y controles de seguridad establecidos?\n",
            "•  Cultura organizativa: ¿Es su organización relajada en cuanto a cómo, cuándo y dónde trabaja la gente, o extremadamente reglamentada?\n",
            "•  Gestión: ¿Existen canales y procesos de comunicación claros entre los principales responsables de la toma de decisiones y el resto de la organización?\n",
            "•  Tamaño de los recursos: ¿Trabaja con un equipo de seguridad de la información o lo hace todo una persona?\n",
            "•  Madurez de los recursos: ¿Los recursos disponibles están informados, plenamente formados, son fiables y constantes, o el personal carece de experiencia y cambia constantemente?\n",
            "•  Formatos de los activos de información: ¿Sus activos de información se almacenan principalmente en formato impreso o electrónicamente en un servidor o en sistemas remotos basados en\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Configuración Tesseract OCR\n",
        "pytesseract.pytesseract.tesseract_cmd = '/usr/bin/tesseract'\n",
        "nlp = spacy.load(\"en_core_web_sm\")"
      ],
      "metadata": {
        "id": "x1UQEEwVMOPu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#Pipeline modelo + OCR\n",
        "\n",
        "# 🔹 API Key de VirusTotal\n",
        "API_KEY = \"06858db9f480b4aba21a5831457a9b919b1f9014e6f8872ee1f4f7d1a029197c\"\n",
        "HEADERS = {\"x-apikey\": API_KEY}\n",
        "\n",
        "def preprocesar_imagen(imagen):\n",
        "    \"\"\"Convierte la imagen a escala de grises y mejora el contraste.\"\"\"\n",
        "    try:\n",
        "        image = Image.open(imagen)\n",
        "        image = image.convert(\"L\")\n",
        "        enhancer = ImageEnhance.Contrast(image)\n",
        "        image = enhancer.enhance(2.0)\n",
        "        return image\n",
        "    except Exception as e:\n",
        "        return None\n",
        "\n",
        "def extraer_texto_img(imagen):\n",
        "    \"\"\"Extrae texto de una imagen tras preprocesarla.\"\"\"\n",
        "    try:\n",
        "        image = preprocesar_imagen(imagen)\n",
        "        if image is None:\n",
        "            return \"Error al procesar la imagen\"\n",
        "        # Extraer texto con pytesseract\n",
        "        texto_extraido = pytesseract.image_to_string(image)\n",
        "        return limpiar_texto(texto_extraido)\n",
        "    except Exception as e:\n",
        "        return f\"Error al procesar la imagen: {str(e)}\"\n",
        "\n",
        "def consultar_ip(ip):\n",
        "    \"\"\"Consulta una IP en VirusTotal y evalúa si es segura o maliciosa.\"\"\"\n",
        "    url = f\"https://www.virustotal.com/api/v3/ip_addresses/{ip}\"\n",
        "    response = requests.get(url, headers=HEADERS)\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        data = response.json()\n",
        "        stats = data[\"data\"][\"attributes\"][\"last_analysis_stats\"]\n",
        "        malicious = stats.get(\"malicious\", 0)\n",
        "        harmless = stats.get(\"harmless\", 0)\n",
        "\n",
        "        if malicious > 0:\n",
        "            veredicto = f\"❌ La IP {ip} ha sido reportada como **maliciosa** en {malicious} análisis.\"\n",
        "        else:\n",
        "            veredicto = f\"✅ La IP {ip} parece **segura**, sin reportes de actividad maliciosa.\"\n",
        "\n",
        "        return {\n",
        "            \"IP\": ip,\n",
        "            \"Veredicto\": veredicto,\n",
        "            \"Análisis\": stats\n",
        "        }\n",
        "    return {\"error\": f\"Error en la consulta: {response.status_code}\"}\n",
        "\n",
        "def consultar_url(url):\n",
        "    \"\"\"Consulta una URL en VirusTotal y evalúa si es segura o maliciosa.\"\"\"\n",
        "    scan_url = \"https://www.virustotal.com/api/v3/urls\"\n",
        "    response = requests.post(scan_url, headers=HEADERS, data={\"url\": url})\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        analysis_id = response.json()[\"data\"][\"id\"]\n",
        "        result_url = f\"https://www.virustotal.com/api/v3/analyses/{analysis_id}\"\n",
        "        result_response = requests.get(result_url, headers=HEADERS)\n",
        "\n",
        "        if result_response.status_code == 200:\n",
        "            data = result_response.json()\n",
        "            stats = data[\"data\"][\"attributes\"][\"stats\"]\n",
        "            malicious = stats.get(\"malicious\", 0)\n",
        "            harmless = stats.get(\"harmless\", 0)\n",
        "\n",
        "            if malicious > 0:\n",
        "                veredicto = f\"❌ La URL {url} ha sido **marcada como maliciosa** en {malicious} análisis.\"\n",
        "            else:\n",
        "                veredicto = f\"✅ La URL {url} parece **segura**, sin reportes de actividad maliciosa.\"\n",
        "\n",
        "            return {\n",
        "                \"URL\": url,\n",
        "                \"Veredicto\": veredicto,\n",
        "                \"Análisis\": stats\n",
        "            }\n",
        "\n",
        "    return {\"error\": f\"Error en la consulta: {response.status_code}\"}\n",
        "\n",
        "def analizar_prompt(prompt):\n",
        "    \"\"\"Detecta si el prompt contiene una IP o URL y consulta VirusTotal si es necesario.\"\"\"\n",
        "\n",
        "    if isinstance(prompt, str) and (prompt.lower().endswith(('.png', '.jpg', '.jpeg', '.gif', '.bmp', '.webp', '.heic'))):\n",
        "        print(f\"🔍 Detectada imagen: {prompt}\")\n",
        "        texto_extraido = extraer_texto_img(prompt)\n",
        "        print(f\"Texto extraído de la imagen: {texto_extraido}\")\n",
        "\n",
        "        return analizar_con_modelo(texto_extraido)\n",
        "\n",
        "    ip_pattern = r\"\\b(?:\\d{1,3}\\.){3}\\d{1,3}\\b\"\n",
        "    url_pattern = r'(https?://[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}(?:/[^ \\n]*)?|www\\.[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}(?:/[^ \\n]*)?|[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}(?:/[^ \\n]*)?)'\n",
        "\n",
        "    ip_match = re.search(ip_pattern, prompt)\n",
        "    url_match = re.search(url_pattern, prompt)\n",
        "\n",
        "    if ip_match:\n",
        "        ip = ip_match.group()\n",
        "        print(f\"🔍 Detectada IP en el prompt: {ip}\")\n",
        "        return consultar_ip(ip)\n",
        "\n",
        "    if url_match:\n",
        "        url = url_match.group()\n",
        "        print(f\"🔍 Detectada URL en el prompt: {url}\")\n",
        "        return consultar_url(url)\n",
        "\n",
        "    return None  # No se detectó ninguna IP o URL\n",
        "\n",
        "def generar_respuesta(prompt):\n",
        "    \"\"\"Genera una respuesta con el modelo o consulta VirusTotal si es necesario.\"\"\"\n",
        "\n",
        "    resultado_api = analizar_prompt(prompt)\n",
        "\n",
        "    if resultado_api:\n",
        "        return json.dumps(resultado_api, indent=4, ensure_ascii=False)  # Respuesta en JSON\n",
        "\n",
        "    # Tokenizar el prompt y generar la respuesta\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
        "\n",
        "    outputs = model.generate(\n",
        "        inputs.input_ids,\n",
        "        attention_mask=inputs.attention_mask,\n",
        "        max_new_tokens=300,\n",
        "        temperature=0.3,\n",
        "        do_sample=True,\n",
        "        top_k=50,\n",
        "        top_p=0.9,\n",
        "        repetition_penalty=1.5,\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        "    )\n",
        "\n",
        "    # Decodificar y limpiar la salida\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True).strip()\n",
        "    return response.replace(prompt, \"\").strip()\n",
        "\n",
        "def analizar_con_modelo(texto_extraido):\n",
        "    \"\"\"Pasa el texto al modelo para que lo analice en busca de señales claras de phishing.\"\"\"\n",
        "\n",
        "    texto_limpio = limpiar_texto(texto_extraido)\n",
        "\n",
        "    system_prompt = \"\"\"\n",
        "      You are a highly specialized AI in cybersecurity. Your primary task is to analyze messages and detect phishing attempts with precision.\n",
        "\n",
        "      🔹 STRICT RULES:\n",
        "      1. Do not invent information. You must base your response ONLY on the given text.\n",
        "      2. Do not provide general explanations unless explicitly asked.\n",
        "      3. Be concise and precise. Your response must be short and strictly relevant.\n",
        "      4. Use formal cybersecurity language and avoid assumptions.\n",
        "      5. Output must always start with either \"Phishing\" or \"Not Phishing\", followed by a brief explanation of why.\n",
        "\n",
        "      🔹 HOW TO ANALYZE A MESSAGE FOR PHISHING:\n",
        "      - Urgency: Does the message pressure the user to act quickly?\n",
        "      - Suspicious Links: Does it contain shortened or untrusted links?\n",
        "      - Requests for Personal Information: Is the user asked to provide passwords or sensitive data?\n",
        "      - Errors or Inconsistencies: Are there grammar mistakes, unnatural tone, or unusual sender details?\n",
        "\n",
        "      🔹 RESPONSE FORMAT (STRICT):\n",
        "      ```\n",
        "      Phishing or Not Phishing\n",
        "      Explanation: [Concise justification, mentioning phishing indicators if present]\n",
        "      ```\n",
        "      ---\n",
        "    \"\"\"\n",
        "\n",
        "    prompt_modelo = f\"\"\"\n",
        "  \t  Analyze the following message and determine whether it is a **phishing attempt** based on the criteria defined in the system instructions.\n",
        "\n",
        "      REMEMBER: Your response must strictly follow the required format. Do not repeat instructions or add unnecessary details.\n",
        "\n",
        "      MESSAGE TO EVALUATE:\n",
        "      {texto_extraido}\n",
        "    \"\"\"\n",
        "\n",
        "    # Tokenizamos el prompt y lo pasamos al modelo\n",
        "    inputs = tokenizer(system_prompt + prompt_modelo, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
        "\n",
        "    outputs = model.generate(\n",
        "        inputs.input_ids,\n",
        "        attention_mask=inputs.attention_mask,\n",
        "        max_new_tokens=300,  # Limitar los tokens de la respuesta\n",
        "        temperature=0.15,  # Controlar la aleatoriedad\n",
        "        do_sample=True,  # Sin aleatorización\n",
        "        top_k=10,  # Menos diversidad\n",
        "        top_p=0.7,\n",
        "        repetition_penalty=1.2,\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        "    )\n",
        "\n",
        "    # Decodificar y limpiar la salida\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True).strip()\n",
        "\n",
        "    if \"</think>\" in response:\n",
        "        user_answer = response.split(\"</think>\", 1)[-1].strip()\n",
        "\n",
        "    return user_answer\n",
        "\n",
        "\n",
        "\n",
        "def limpiar_texto(texto):\n",
        "    \"\"\"Limpia el texto extraído de caracteres no deseados.\"\"\"\n",
        "    texto = texto.strip()\n",
        "    texto = re.sub(r'\\s+', ' ', texto)  # Reemplazar espacios extra\n",
        "    texto = texto.replace(\"\\n\", \" \").replace(\"\\r\", \"\")  # Eliminar saltos de línea\n",
        "    return texto\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "prompt_usuario_4 = 'image6.png'\n",
        "respuesta_4 = generar_respuesta(prompt_usuario_4)\n",
        "print(f\"\\n🔹 Respuesta para imagen:\\n{respuesta_4}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "GVoVeJpjhE0h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "382a9d81-aece-4ec6-d083-879650727675"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔍 Detectada imagen: image6.png\n",
            "Texto extraído de la imagen: Sent on: Friday, June 23, 2023 11:31:12 AM To: Subject: YOUR ACCOUNT IS AT RISK!! Dear Valued User , We received a request from you to terminate your Office 365 email due to a dual college/universities account. This process has begun by our administrator. If you did not authorize this action and you have no knowledge of it, you are advised to re-verify your account. Please give us 24 hours to terminate your account if you initiated the request. Failure to re-verify will result in the closure of your account and you will lose all of my files on these 365 accounts. If this request was made accidentally and you have no knowledge of it, you are advised to copy and paste the URL Below into the address bar of your web browser to fill in the form. cutt.ly/OwtNi6KO Failure to Verify will result in the closure of your account. lowa State University IT Helpdesk All Right Reserved.\n",
            "\n",
            "🔹 Respuesta para imagen:\n",
            "\"Not Phishing  \\nExplanation: The message does not exhibit urgency, suspicious links, requests for personal information, errors, or inconsistencies that would indicate a phishing attempt.\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Función para generar respuestas\n",
        "def generate_response(text):\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
        "    output = model.generate(**inputs, max_new_tokens=200, temperature=0.2, do_sample=True)\n",
        "    return tokenizer.decode(output[0], skip_special_tokens=True)"
      ],
      "metadata": {
        "id": "v1nHM3I1d8l5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nest_asyncio\n",
        "nest_asyncio.apply()"
      ],
      "metadata": {
        "id": "6hUB9XOPrNkc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cargar variables de entorno\n",
        "load_dotenv()\n",
        "\n",
        "# Configurar logging\n",
        "logging.basicConfig(\n",
        "    format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\",\n",
        "    level=logging.INFO,\n",
        "    handlers=[logging.StreamHandler(sys.stdout)]\n",
        ")\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Handlers de Telegram\n",
        "async def start(update: Update, context: CallbackContext) -> None:\n",
        "    await update.message.reply_text(\"¡Hola! Soy tu especialista en ciberseguridad. ¿En qué te puedo ayudar?\")\n",
        "\n",
        "async def handle_message(update: Update, context: CallbackContext) -> None:\n",
        "    user_input = update.message.text\n",
        "    username = update.message.from_user.username\n",
        "    logger.info(f\"Mensaje recibido de {username}: {user_input}\")\n",
        "\n",
        "    response = generate_response(user_input)  # Usa la función importada\n",
        "    await update.message.reply_text(response)\n",
        "\n",
        "# Función principal\n",
        "def main():\n",
        "    TELEGRAM_TOKEN = \"7047664203:AAEa-JEcZQpv-tDCIdV6ZE_odp4lPTH0Bd8\"\n",
        "    if not TELEGRAM_TOKEN:\n",
        "        logger.error(\"El token de Telegram no está configurado.\")\n",
        "        sys.exit(1)\n",
        "\n",
        "    app = Application.builder().token(TELEGRAM_TOKEN).build()\n",
        "    app.add_handler(CommandHandler(\"start\", start))\n",
        "    app.add_handler(MessageHandler(filters.TEXT & ~filters.COMMAND, handle_message))\n",
        "\n",
        "    logger.info(\"Bot iniciado y ejecutándose...\")\n",
        "    app.run_polling()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "5VZRU4Htd88x",
        "outputId": "a1ed7efc-fd94-425c-eb1a-e470124c119c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Cannot close a running event loop",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-b9bebe5f3a3c>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-29-b9bebe5f3a3c>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Bot iniciado y ejecutándose...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m     \u001b[0mapp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_polling\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/telegram/ext/_application.py\u001b[0m in \u001b[0;36mrun_polling\u001b[0;34m(self, poll_interval, timeout, bootstrap_retries, read_timeout, write_timeout, connect_timeout, pool_timeout, allowed_updates, drop_pending_updates, close_loop, stop_signals)\u001b[0m\n\u001b[1;32m    871\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_task\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m         return self.__run(\n\u001b[0m\u001b[1;32m    874\u001b[0m             updater_coroutine=self.updater.start_polling(\n\u001b[1;32m    875\u001b[0m                 \u001b[0mpoll_interval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpoll_interval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/telegram/ext/_application.py\u001b[0m in \u001b[0;36m__run\u001b[0;34m(self, updater_coroutine, stop_signals, bootstrap_retries, close_loop)\u001b[0m\n\u001b[1;32m   1115\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1116\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mclose_loop\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1117\u001b[0;31m                     \u001b[0mloop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m     def create_task(\n",
            "\u001b[0;32m/usr/lib/python3.11/asyncio/unix_events.py\u001b[0m in \u001b[0;36mclose\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_finalizing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0msig\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_signal_handlers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/asyncio/selector_events.py\u001b[0m in \u001b[0;36mclose\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_running\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Cannot close a running event loop\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Cannot close a running event loop"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "main()"
      ],
      "metadata": {
        "id": "3X-GOoxBsqz9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}